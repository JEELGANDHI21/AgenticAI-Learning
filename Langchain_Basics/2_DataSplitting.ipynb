{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91a05e35",
   "metadata": {},
   "source": [
    "Data Splitting\n",
    "\n",
    "Documents to Text Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b0f0942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3'}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V) = softmax(QKT\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈ Rdmodel×dk , WK\\ni ∈ Rdmodel×dk , WV\\ni ∈ Rdmodel×dv\\nand WO ∈ Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 · d) O(1) O(1)\\nRecurrent O(n · d2) O(n) O(n)\\nConvolutional O(k · n · d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r · n · d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nP E(pos,2i) = sin(pos/100002i/dmodel )\\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\\nP Epos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}, page_content='length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < ndoes not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0 · 1020\\nGNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020\\nConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020\\nMoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.8 2.3 · 1019\\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10'}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}, page_content='[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader('attention.pdf')\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48c0e711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "page_content='Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez∗ †\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "Łukasz Kaiser∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin∗ ‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring significantly\n",
      "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n",
      "to-German translation task, improving over the existing best results, including\n",
      "ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n",
      "our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\n",
      "training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n",
      "best models from the literature. We show that the Transformer generalizes well to\n",
      "other tasks by applying it successfully to English constituency parsing both with\n",
      "large and limited training data.\n",
      "∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
      "the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\n",
      "has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
      "attention and the parameter-free position representation and became the other person involved in nearly every\n",
      "detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
      "tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\n",
      "efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\n",
      "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\n",
      "our research.\n",
      "†Work performed while at Google Brain.\n",
      "‡Work performed while at Google Research.\n",
      "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\n",
      "arXiv:1706.03762v7  [cs.CL]  2 Aug 2023' metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "print(type(docs))\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360a0f83",
   "metadata": {},
   "source": [
    "1. Recursively split text by characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55c44d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50) \n",
    "chunk_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa65b25d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='University of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='based solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='architectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='sequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='tion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='The Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='block, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='used successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='language modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3'}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3'}, page_content='wise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3'}, page_content='Decoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3'}, page_content='masking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V) = softmax(QKT\\n√dk\\n)V (1)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='Attention(Q, K, V) = softmax(QKT\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='much faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='extremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈ Rdmodel×dk , WK\\ni ∈ Rdmodel×dk , WV'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}, page_content='i ∈ Rdmodel×dk , WK\\ni ∈ Rdmodel×dk , WV\\ni ∈ Rdmodel×dv\\nand WO ∈ Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}, page_content='• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}, page_content='and queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}, page_content='inside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}, page_content='FFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}, page_content='tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 · d) O(1) O(1)\\nRecurrent O(n · d2) O(n) O(n)\\nConvolutional O(k · n · d2) O(1) O(logk(n))'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='Convolutional O(k · n · d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r · n · d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nP E(pos,2i) = sin(pos/100002i/dmodel )\\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\\nP Epos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='The third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='the maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}, page_content='length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}, page_content='path length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < ndoes not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}, page_content='recurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}, page_content='from our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}, page_content='sentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}, page_content='target tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}, page_content='(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4 Regularization'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}, page_content='warmup_steps = 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0 · 1020\\nGNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020\\nConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020\\nMoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}, page_content='Deep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.8 2.3 · 1019\\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}, page_content='positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}, page_content='in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}, page_content='the competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}, page_content='were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}, page_content='architectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}, page_content='5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}, page_content='(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}, page_content='big 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}, page_content='In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}, page_content='sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}, page_content='models have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}, page_content='for the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10'}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10'}, page_content='Zhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10'}, page_content='Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10'}, page_content='7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10'}, page_content='English-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10'}, page_content='such as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10'}, page_content='arXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}, page_content='[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}, page_content='of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}, page_content='arXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}, page_content='9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}, page_content='[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}, page_content='2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}, page_content='Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}, page_content='model. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}, page_content='2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}, page_content='layer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}, page_content='Advances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}, page_content='[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}, page_content='arXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13'}, page_content='.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14'}, page_content='but\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15'}, page_content='but\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e14f9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a0af17",
   "metadata": {},
   "source": [
    "Splitting from String -> Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e54f8fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Transformer model, introduced in the paper “Attention is All You Need” by Vaswani et al. in 2017, marked a significant departure from previous deep learning architectures used for natural language processing (NLP).\\n\\nPrior to Transformers, models like Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks (LSTMs) were the go-to methods for handling sequential data. \\n\\nHowever, these models processed data step by step, leading to slower training times and limitations in capturing long-range dependencies. \\n\\nThe Transformer, in contrast, uses a mechanism called self-attention that enables it to process entire sequences of data in parallel, making it much faster and more effective at capturing complex relationships between words, regardless of their distance in the sequence.\\n\\nAt the heart of the Transformer architecture lies the self-attention mechanism. Self-attention allows each token in a sequence to interact with every other token in the sequence, computing a weighted representation of the input sequence. \\n\\nThis mechanism helps the model understand contextual relationships between words even if they are far apart. \\n\\nFor example, in the sentence “The cat sat on the mat,” the word “cat” is highly relevant to “sat,” but less so to “mat.” Self-attention allows the model to assign higher attention to \"cat\" and \"sat\" than \"cat\" and \"mat.\" \\n\\nThis ability to weigh token importance dynamically is what gives Transformers their superior performance in NLP tasks\\n\\nThe Transformer model follows an encoder-decoder architecture, which can be seen in models like Machine Translation where an input sentence in one language is encoded and then decoded into another language. The encoder is responsible for processing the input sequence and producing a set of representations. \\n\\nThe decoder, on the other hand, takes this encoded information and generates the output sequence. Both the encoder and decoder consist of multiple layers of self-attention and feed-forward neural networks. \\n\\nThe encoder’s layers work to capture the relationships in the input, while the decoder generates the output while attending to both the encoder’s output and previous tokens of the output sequence.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str = \"\"\n",
    "with open('aboutTransformer.txt', encoding=\"utf-8\") as f:\n",
    "    str = f.read()\n",
    "\n",
    "str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e52a8778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f4a4ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='The Transformer model, introduced in the paper “Attention is All You Need” by Vaswani et al. in'),\n",
       " Document(metadata={}, page_content='et al. in 2017, marked a significant departure from previous deep learning architectures used for'),\n",
       " Document(metadata={}, page_content='used for natural language processing (NLP).'),\n",
       " Document(metadata={}, page_content='Prior to Transformers, models like Recurrent Neural Networks (RNNs) and Long Short-Term Memory'),\n",
       " Document(metadata={}, page_content='Memory networks (LSTMs) were the go-to methods for handling sequential data.'),\n",
       " Document(metadata={}, page_content='However, these models processed data step by step, leading to slower training times and limitations'),\n",
       " Document(metadata={}, page_content='in capturing long-range dependencies.'),\n",
       " Document(metadata={}, page_content='The Transformer, in contrast, uses a mechanism called self-attention that enables it to process'),\n",
       " Document(metadata={}, page_content='process entire sequences of data in parallel, making it much faster and more effective at capturing'),\n",
       " Document(metadata={}, page_content='capturing complex relationships between words, regardless of their distance in the sequence.'),\n",
       " Document(metadata={}, page_content='At the heart of the Transformer architecture lies the self-attention mechanism. Self-attention'),\n",
       " Document(metadata={}, page_content='allows each token in a sequence to interact with every other token in the sequence, computing a'),\n",
       " Document(metadata={}, page_content='a weighted representation of the input sequence.'),\n",
       " Document(metadata={}, page_content='This mechanism helps the model understand contextual relationships between words even if they are'),\n",
       " Document(metadata={}, page_content='they are far apart.'),\n",
       " Document(metadata={}, page_content='For example, in the sentence “The cat sat on the mat,” the word “cat” is highly relevant to “sat,”'),\n",
       " Document(metadata={}, page_content='to “sat,” but less so to “mat.” Self-attention allows the model to assign higher attention to \"cat\"'),\n",
       " Document(metadata={}, page_content='to \"cat\" and \"sat\" than \"cat\" and \"mat.\"'),\n",
       " Document(metadata={}, page_content='This ability to weigh token importance dynamically is what gives Transformers their superior'),\n",
       " Document(metadata={}, page_content='superior performance in NLP tasks'),\n",
       " Document(metadata={}, page_content='The Transformer model follows an encoder-decoder architecture, which can be seen in models like'),\n",
       " Document(metadata={}, page_content='like Machine Translation where an input sentence in one language is encoded and then decoded into'),\n",
       " Document(metadata={}, page_content='into another language. The encoder is responsible for processing the input sequence and producing a'),\n",
       " Document(metadata={}, page_content='a set of representations.'),\n",
       " Document(metadata={}, page_content='The decoder, on the other hand, takes this encoded information and generates the output sequence.'),\n",
       " Document(metadata={}, page_content='sequence. Both the encoder and decoder consist of multiple layers of self-attention and'),\n",
       " Document(metadata={}, page_content='and feed-forward neural networks.'),\n",
       " Document(metadata={}, page_content='The encoder’s layers work to capture the relationships in the input, while the decoder generates'),\n",
       " Document(metadata={}, page_content='generates the output while attending to both the encoder’s output and previous tokens of the output'),\n",
       " Document(metadata={}, page_content='output sequence.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=10)\n",
    "chunk_text = text_splitter.create_documents([str])\n",
    "chunk_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5711cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content='The Transformer model, introduced in the paper “Attention is All You Need” by Vaswani et al. in')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c41c01",
   "metadata": {},
   "source": [
    "2. Character Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27087031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'aboutTransformer.txt'}, page_content='The Transformer model, introduced in the paper “Attention is All You Need” by Vaswani et al. in 2017, marked a significant departure from previous deep learning architectures used for natural language processing (NLP).\\n\\nPrior to Transformers, models like Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks (LSTMs) were the go-to methods for handling sequential data. \\n\\nHowever, these models processed data step by step, leading to slower training times and limitations in capturing long-range dependencies. \\n\\nThe Transformer, in contrast, uses a mechanism called self-attention that enables it to process entire sequences of data in parallel, making it much faster and more effective at capturing complex relationships between words, regardless of their distance in the sequence.\\n\\nAt the heart of the Transformer architecture lies the self-attention mechanism. Self-attention allows each token in a sequence to interact with every other token in the sequence, computing a weighted representation of the input sequence. \\n\\nThis mechanism helps the model understand contextual relationships between words even if they are far apart. \\n\\nFor example, in the sentence “The cat sat on the mat,” the word “cat” is highly relevant to “sat,” but less so to “mat.” Self-attention allows the model to assign higher attention to \"cat\" and \"sat\" than \"cat\" and \"mat.\" \\n\\nThis ability to weigh token importance dynamically is what gives Transformers their superior performance in NLP tasks\\n\\nThe Transformer model follows an encoder-decoder architecture, which can be seen in models like Machine Translation where an input sentence in one language is encoded and then decoded into another language. The encoder is responsible for processing the input sequence and producing a set of representations. \\n\\nThe decoder, on the other hand, takes this encoded information and generates the output sequence. Both the encoder and decoder consist of multiple layers of self-attention and feed-forward neural networks. \\n\\nThe encoder’s layers work to capture the relationships in the input, while the decoder generates the output while attending to both the encoder’s output and previous tokens of the output sequence.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader \n",
    "loader = TextLoader(\"aboutTransformer.txt\", encoding='utf-8')\n",
    "text_docs = loader.load()\n",
    "text_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f526532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 218, which is longer than the specified 50\n",
      "Created a chunk of size 165, which is longer than the specified 50\n",
      "Created a chunk of size 138, which is longer than the specified 50\n",
      "Created a chunk of size 270, which is longer than the specified 50\n",
      "Created a chunk of size 238, which is longer than the specified 50\n",
      "Created a chunk of size 109, which is longer than the specified 50\n",
      "Created a chunk of size 221, which is longer than the specified 50\n",
      "Created a chunk of size 117, which is longer than the specified 50\n",
      "Created a chunk of size 308, which is longer than the specified 50\n",
      "Created a chunk of size 206, which is longer than the specified 50\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter \n",
    "text_splitter = CharacterTextSplitter(separator=\"\\n\\n\", chunk_size=50, chunk_overlap=20)\n",
    "chunks = text_splitter.split_documents(text_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f514222b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'aboutTransformer.txt'}, page_content='The Transformer model, introduced in the paper “Attention is All You Need” by Vaswani et al. in 2017, marked a significant departure from previous deep learning architectures used for natural language processing (NLP).')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "669276f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 218, which is longer than the specified 50\n",
      "Created a chunk of size 165, which is longer than the specified 50\n",
      "Created a chunk of size 138, which is longer than the specified 50\n",
      "Created a chunk of size 270, which is longer than the specified 50\n",
      "Created a chunk of size 238, which is longer than the specified 50\n",
      "Created a chunk of size 109, which is longer than the specified 50\n",
      "Created a chunk of size 221, which is longer than the specified 50\n",
      "Created a chunk of size 117, which is longer than the specified 50\n",
      "Created a chunk of size 308, which is longer than the specified 50\n",
      "Created a chunk of size 206, which is longer than the specified 50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='The Transformer model, introduced in the paper “Attention is All You Need” by Vaswani et al. in 2017, marked a significant departure from previous deep learning architectures used for natural language processing (NLP).'),\n",
       " Document(metadata={}, page_content='Prior to Transformers, models like Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks (LSTMs) were the go-to methods for handling sequential data.'),\n",
       " Document(metadata={}, page_content='However, these models processed data step by step, leading to slower training times and limitations in capturing long-range dependencies.'),\n",
       " Document(metadata={}, page_content='The Transformer, in contrast, uses a mechanism called self-attention that enables it to process entire sequences of data in parallel, making it much faster and more effective at capturing complex relationships between words, regardless of their distance in the sequence.'),\n",
       " Document(metadata={}, page_content='At the heart of the Transformer architecture lies the self-attention mechanism. Self-attention allows each token in a sequence to interact with every other token in the sequence, computing a weighted representation of the input sequence.'),\n",
       " Document(metadata={}, page_content='This mechanism helps the model understand contextual relationships between words even if they are far apart.'),\n",
       " Document(metadata={}, page_content='For example, in the sentence “The cat sat on the mat,” the word “cat” is highly relevant to “sat,” but less so to “mat.” Self-attention allows the model to assign higher attention to \"cat\" and \"sat\" than \"cat\" and \"mat.\"'),\n",
       " Document(metadata={}, page_content='This ability to weigh token importance dynamically is what gives Transformers their superior performance in NLP tasks'),\n",
       " Document(metadata={}, page_content='The Transformer model follows an encoder-decoder architecture, which can be seen in models like Machine Translation where an input sentence in one language is encoded and then decoded into another language. The encoder is responsible for processing the input sequence and producing a set of representations.'),\n",
       " Document(metadata={}, page_content='The decoder, on the other hand, takes this encoded information and generates the output sequence. Both the encoder and decoder consist of multiple layers of self-attention and feed-forward neural networks.'),\n",
       " Document(metadata={}, page_content='The encoder’s layers work to capture the relationships in the input, while the decoder generates the output while attending to both the encoder’s output and previous tokens of the output sequence.')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text_splitter.create_documents([str])\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35338a74",
   "metadata": {},
   "source": [
    "3. HTML Header Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3b8f4a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header1': 'Hello, World!'}, page_content='Hello, World!'),\n",
       " Document(metadata={'Header1': 'Hello, World!'}, page_content='This is a Hello World paragraph.'),\n",
       " Document(metadata={'Header1': 'Hello, World!', 'Header2': 'Hello, Jeel!'}, page_content='Hello, Jeel!'),\n",
       " Document(metadata={'Header1': 'Hello, World!', 'Header2': 'Hello, Jeel!'}, page_content='This is a Hello Shubh paragraph.  \\nHello, Langchain!  \\nThis is a Hello Langchain paragraph..')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import HTMLHeaderTextSplitter \n",
    "html_string = \"\"\"<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Simple HTML Example</title>\n",
    "</head>\n",
    "<body>\n",
    "    <div>\n",
    "        <h1>Hello, World!</h1>\n",
    "        <p>This is a Hello World paragraph.</p>   \n",
    "    </div>\n",
    "    <div>\n",
    "        <h2>Hello, Jeel!</h2>\n",
    "        <p>This is a Hello Shubh paragraph.</p>   \n",
    "    </div>\n",
    "    <div>\n",
    "        <h3>Hello, Langchain!</h3>\n",
    "        <p>This is a Hello Langchain paragraph..</p>   \n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "header_to_split_on= [\n",
    "    (\"h1\", \"Header1\"),\n",
    "    (\"h2\", \"Header2\")\n",
    "]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(header_to_split_on)\n",
    "html_header_splits = html_splitter.split_text(html_string)\n",
    "html_header_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40a3db9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='Jump to content  \\nMain menu  \\nMain menu  \\nmove to sidebar  \\nhide  \\nNavigation  \\nMain page  \\nContents  \\nCurrent events  \\nRandom article  \\nAbout Wikipedia  \\nContact us  \\nContribute  \\nHelp  \\nLearn to edit  \\nCommunity portal  \\nRecent changes  \\nUpload file  \\nSpecial pages  \\nSearch  \\nSearch  \\nAppearance  \\nDonate  \\nCreate account  \\nLog in  \\nPersonal tools  \\nDonate  \\nCreate account  \\nLog in  \\nPages for logged out editors  \\nlearn more  \\nContributions  \\nTalk  \\nCentralNotice'),\n",
       " Document(metadata={'Header2': 'Contents'}, page_content='Contents'),\n",
       " Document(metadata={}, page_content=\"move to sidebar  \\nhide  \\n(Top)  \\n1  \\nPrinciples  \\nToggle Principles subsection  \\n1.1  \\nIdeal transformer  \\n1.2  \\nReal transformer  \\n1.2.1  \\nDeviations from ideal transformer  \\n1.2.2  \\nLeakage flux  \\n1.2.3  \\nEquivalent circuit  \\n1.3  \\nTransformer EMF equation  \\n1.4  \\nPolarity  \\n1.5  \\nEffect of frequency  \\n1.6  \\nEnergy losses  \\n2  \\nConstruction  \\nToggle Construction subsection  \\n2.1  \\nCores  \\n2.1.1  \\nLaminated steel cores  \\n2.1.2  \\nSolid cores  \\n2.1.3  \\nToroidal cores  \\n2.1.4  \\nAir cores  \\n2.2  \\nWindings  \\n2.3  \\nCooling  \\n2.4  \\nInsulation  \\n2.5  \\nBushings  \\n3  \\nClassification parameters  \\n4  \\nApplications  \\n5  \\nHistory  \\nToggle History subsection  \\n5.1  \\nDiscovery of induction  \\n5.2  \\nInduction coils  \\n5.3  \\nFirst alternating current transformers  \\n5.3.1  \\nEarly series circuit transformer distribution  \\n5.4  \\nClosed-core transformers and parallel power distribution  \\n5.5  \\nWestinghouse improvements  \\n5.6  \\nOther early transformer designs  \\n6  \\nSee also  \\n7  \\nNotes  \\n8  \\nReferences  \\n9  \\nBibliography  \\n10  \\nExternal links  \\nToggle the table of contents  \\nTransformer  \\n113 languages  \\nAfrikaans  \\nAlemannisch  \\nالعربية  \\nAragonés  \\nঅসমীয়া  \\nAsturianu  \\nAzərbaycanca  \\nتۆرکجه  \\nবাংলা  \\n閩南語 / Bân-lâm-gí  \\nБашҡортса  \\nБеларуская  \\nБеларуская (тарашкевіца)  \\nБългарски  \\nBoarisch  \\nBosanski  \\nCatalà  \\nČeština  \\nCorsu  \\nCymraeg  \\nDansk  \\nDeutsch  \\nEesti  \\nΕλληνικά  \\nEspañol  \\nEsperanto  \\nEuskara  \\nفارسی  \\nFiji Hindi  \\nFrançais  \\nGaeilge  \\nGalego  \\n한국어  \\nՀայերեն  \\nहिन्दी  \\nHrvatski  \\nIdo  \\nBahasa Indonesia  \\nInterlingua  \\nIsiZulu  \\nÍslenska  \\nItaliano  \\nעברית  \\nJawa  \\nಕನ್ನಡ  \\nქართული  \\nҚазақша  \\nKiswahili  \\nKreyòl ayisyen  \\nКыргызча  \\nLatina  \\nLatviešu  \\nLietuvių  \\nLombard  \\nMagyar  \\nМакедонски  \\nMalagasy  \\nമലയാളം  \\nमराठी  \\nBahasa Melayu  \\nMinangkabau  \\nМонгол  \\nမြန်မာဘာသာ  \\nNederlands  \\nनेपाली  \\nनेपाल भाषा  \\n日本語  \\nНохчийн  \\nNordfriisk  \\nNorsk bokmål  \\nNorsk nynorsk  \\nOccitan  \\nଓଡ଼ିଆ  \\nOʻzbekcha / ўзбекча  \\nਪੰਜਾਬੀ  \\nپنجابی  \\nPiemontèis  \\nPolski  \\nPortuguês  \\nQaraqalpaqsha  \\nRomână  \\nРусиньскый  \\nРусский  \\nScots  \\nSeeltersk  \\nShqip  \\nSimple English  \\nسنڌي  \\nSlovenčina  \\nSlovenščina  \\nکوردی  \\nСрпски / srpski  \\nSrpskohrvatski / српскохрватски  \\nSunda  \\nSuomi  \\nSvenska  \\nTagalog  \\nதமிழ்  \\nТатарча / tatarça  \\nతెలుగు  \\nไทย  \\nТоҷикӣ  \\nTürkçe  \\nTürkmençe  \\nУкраїнська  \\nاردو  \\nTiếng Việt  \\nWinaray  \\nWolof  \\n吴语  \\nייִדיש  \\n粵語  \\n中文  \\nEdit links  \\nArticle  \\nTalk  \\nEnglish  \\nRead  \\nEdit  \\nView history  \\nTools  \\nTools  \\nmove to sidebar  \\nhide  \\nActions  \\nRead  \\nEdit  \\nView history  \\nGeneral  \\nWhat links here  \\nRelated changes  \\nUpload file  \\nPermanent link  \\nPage information  \\nCite this page  \\nGet shortened URL  \\nDownload QR code  \\nPrint/export  \\nDownload as PDF  \\nPrintable version  \\nIn other projects  \\nWikimedia Commons  \\nWikidata item  \\nAppearance  \\nmove to sidebar  \\nhide  \\nFrom Wikipedia, the free encyclopedia  \\nDevice to couple energy between circuits  \\n.mw-parser-output .hatnote{font-style:italic}.mw-parser-output div.hatnote{padding-left:1.6em;margin-bottom:0.5em}.mw-parser-output .hatnote i{font-style:normal}.mw-parser-output .hatnote+link+.hatnote{margin-top:-0.5em}@media print{body.ns-0 .mw-parser-output .hatnote{display:none!important}}  \\nThis article is about the electrical device. For other uses, see .  \\nTransformer (disambiguation)  \\n.mw-parser-output .infobox-subbox{padding:0;border:none;margin:-3px;width:auto;min-width:100%;font-size:100%;clear:none;float:none;background-color:transparent}.mw-parser-output .infobox-3cols-child{margin:auto}.mw-parser-output .infobox .navbar{font-size:100%}@media screen{html.skin-theme-clientpref-night .mw-parser-output .infobox-full-data:not(.notheme)>div:not(.notheme)[style]{background:#1f1f23!important;color:#f8f9fa}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .infobox-full-data:not(.notheme)>div:not(.notheme)[style]{background:#1f1f23!important;color:#f8f9fa}}@media(min-width:640px){body.skin--responsive .mw-parser-output .infobox-table{display:table!important}body.skin--responsive .mw-parser-output .infobox-table>caption{display:table-caption!important}body.skin--responsive .mw-parser-output .infobox-table>tbody{display:table-row-group}body.skin--responsive .mw-parser-output .infobox-table th,body.skin--responsive .mw-parser-output .infobox-table td{padding-left:inherit;padding-right:inherit}}  \\nTransformer  \\nAn O-core transformer consisting of two coils of copper wire wrapped around a magnetic core  \\nComponent type  \\nPassive  \\nWorking principle  \\n\\u200d  \\nElectromagnetic induction  \\nInventor  \\nMichael Faraday  \\n1  \\n[  \\n]  \\nInvention year  \\n1831  \\nElectronic symbol  \\nIn , a is a that transfers from one electrical circuit to another circuit, or multiple . A varying current in any coil of the transformer produces a varying in the transformer's core, which induces a varying across any other coils wound around the same core. Electrical energy can be transferred between separate coils without a metallic (conductive) connection between the two circuits. , discovered in 1831, describes the induced voltage effect in any coil due to a changing magnetic flux encircled by the coil.  \\nelectrical engineering  \\ntransformer  \\npassive component  \\nelectrical energy  \\ncircuits  \\nmagnetic flux  \\nelectromotive force (EMF)  \\nFaraday's law of induction  \\nTransformers are used to change voltage levels, such transformers being termed step-up or step-down type to increase or decrease voltage level, respectively. Transformers can also be used to provide between circuits as well as to couple stages of signal-processing circuits. Since the invention of the first in 1885, transformers have become essential for the , , and utilization of alternating current electric power. A wide range of transformer designs is encountered in electronic and electric power applications. Transformers range in size from transformers less than a cubic centimeter in volume, to units weighing hundreds of tons used to interconnect the .  \\nAC  \\ngalvanic isolation  \\nconstant-potential transformer  \\ntransmission  \\ndistribution  \\n2  \\n[  \\n]  \\nRF  \\npower grid  \\n.mw-parser-output .toclimit-2 .toclevel-1 ul,.mw-parser-output .toclimit-3 .toclevel-2 ul,.mw-parser-output .toclimit-4 .toclevel-3 ul,.mw-parser-output .toclimit-5 .toclevel-4 ul,.mw-parser-output .toclimit-6 .toclevel-5 ul,.mw-parser-output .toclimit-7 .toclevel-6 ul{display:none}\"),\n",
       " Document(metadata={'Header2': 'Principles'}, page_content='Principles'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\n.mw-parser-output .side-box{margin:4px 0;box-sizing:border-box;border:1px solid #aaa;font-size:88%;line-height:1.25em;background-color:var(--background-color-interactive-subtle,#f8f9fa);display:flow-root}.mw-parser-output .infobox .side-box{font-size:100%}.mw-parser-output .side-box-abovebelow,.mw-parser-output .side-box-text{padding:0.25em 0.9em}.mw-parser-output .side-box-image{padding:2px 0 2px 0.9em;text-align:center}.mw-parser-output .side-box-imageright{padding:2px 0.9em 2px 0;text-align:center}@media(min-width:500px){.mw-parser-output .side-box-flex{display:flex;align-items:center}.mw-parser-output .side-box-text{flex:1;min-width:0}}@media(min-width:720px){.mw-parser-output .side-box{width:238px}.mw-parser-output .side-box-right{clear:right;float:right;margin-left:1em}.mw-parser-output .side-box-left{margin-right:1em}}  \\n.mw-parser-output .plainlist ol,.mw-parser-output .plainlist ul{line-height:inherit;list-style:none;margin:0;padding:0}.mw-parser-output .plainlist ol li,.mw-parser-output .plainlist ul li{margin-bottom:0}  \\nIdeal transformer equations  \\nBy :  \\nFaraday\\'s law of induction  \\n.mw-parser-output table.numblk{border-collapse:collapse;border:none;margin-top:0;margin-right:0;margin-bottom:0}.mw-parser-output table.numblk>tbody>tr>td{vertical-align:middle;padding:0}.mw-parser-output table.numblk>tbody>tr>td:nth-child(2){width:99%}.mw-parser-output table.numblk>tbody>tr>td:nth-child(2)>table{border-collapse:collapse;margin:0;border:none;width:100%}.mw-parser-output table.numblk>tbody>tr>td:nth-child(2)>table>tbody>tr:first-child>td:first-child,.mw-parser-output table.numblk>tbody>tr>td:nth-child(2)>table>tbody>tr:first-child>td:last-child{padding:0 0.4ex}.mw-parser-output table.numblk>tbody>tr>td:nth-child(2)>table>tbody>tr:first-child>td:nth-child(2){width:100%;padding:0}.mw-parser-output table.numblk>tbody>tr>td:nth-child(2)>table>tbody>tr:last-child>td{padding:0}.mw-parser-output table.numblk>tbody>tr>td:last-child{font-weight:bold}.mw-parser-output table.numblk.numblk-raw-n>tbody>tr>td:last-child{font-weight:unset}.mw-parser-output table.numblk>tbody>tr>td:last-child::before{content:\"(\"}.mw-parser-output table.numblk>tbody>tr>td:last-child::after{content:\")\"}.mw-parser-output table.numblk.numblk-raw-n>tbody>tr>td:last-child::before,.mw-parser-output table.numblk.numblk-raw-n>tbody>tr>td:last-child::after{content:none}.mw-parser-output table.numblk>tbody>tr>td{border:none}.mw-parser-output table.numblk.numblk-border>tbody>tr>td{border:thin solid}.mw-parser-output table.numblk>tbody>tr>td:nth-child(2)>table>tbody>tr:first-child>td{border:none}.mw-parser-output table.numblk.numblk-border>tbody>tr>td:nth-child(2)>table>tbody>tr:first-child>td{border:thin solid}.mw-parser-output table.numblk>tbody>tr>td:nth-child(2)>table>tbody>tr:last-child>td{border-left:none;border-right:none;border-bottom:none}.mw-parser-output table.numblk.numblk-border>tbody>tr>td:nth-child(2)>table>tbody>tr:last-child>td{border-left:thin solid;border-right:thin solid;border-bottom:thin solid}.mw-parser-output table.numblk:target{color:var(--color-base,#202122);background-color:#cfe8fd}@media screen{html.skin-theme-clientpref-night .mw-parser-output table.numblk:target{color:var(--color-base,#eaecf0);background-color:#301702}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output table.numblk:target{color:var(--color-base,#eaecf0);background-color:#301702}}  \\nV  \\nP  \\n=  \\n− −  \\nN  \\nP  \\nd  \\nΦ Φ  \\nd  \\nt  \\n{\\\\displaystyle V_{\\\\text{P}}=-N_{\\\\text{P}}{\\\\frac {\\\\mathrm {d} \\\\Phi }{\\\\mathrm {d} t}}}  \\nEq. 1  \\na  \\n[  \\n]  \\n3  \\n[  \\n]  \\nV  \\nS  \\n=  \\n− −  \\nN  \\nS  \\nd  \\nΦ Φ  \\nd  \\nt  \\n{\\\\displaystyle V_{\\\\text{S}}=-N_{\\\\text{S}}{\\\\frac {\\\\mathrm {d} \\\\Phi }{\\\\mathrm {d} t}}}  \\nEq. 2  \\nwhere is the , is the in a winding, dΦ/dt is the of the magnetic flux Φ through one turn of the winding over time ( ), and subscripts and denote primary and secondary.  \\nV  \\n{\\\\displaystyle V}  \\ninstantaneous  \\nvoltage  \\nN  \\n{\\\\displaystyle N}  \\nnumber of turns  \\nderivative  \\nt  \\nP  \\nS  \\nCombining the ratio of eq. 1 and eq. 2:  \\nTurns ratio  \\n=  \\nV  \\nP  \\nV  \\nS  \\n=  \\nN  \\nP  \\nN  \\nS  \\n=  \\na  \\n{\\\\displaystyle ={\\\\frac {V_{\\\\text{P}}}{V_{\\\\text{S}}}}={\\\\frac {N_{\\\\text{P}}}{N_{\\\\text{S}}}}=a}  \\nEq. 3  \\nwhere for a step-up transformer < 1 and for a step-down transformer > 1.  \\na  \\na  \\n4  \\n[  \\n]  \\nBy the law of , , and power are each conserved in the input and output:  \\nconservation of energy  \\napparent  \\nreal  \\nreactive  \\nS  \\n=  \\nI  \\nP  \\nV  \\nP  \\n=  \\nI  \\nS  \\nV  \\nS  \\n{\\\\displaystyle S=I_{\\\\text{P}}V_{\\\\text{P}}=I_{\\\\text{S}}V_{\\\\text{S}}}  \\nEq. 4  \\nwhere is apparent power and is .  \\nS  \\n{\\\\displaystyle S}  \\nI  \\n{\\\\displaystyle I}  \\ncurrent  \\nCombining Eq. 3 and Eq. 4 with this endnote gives the ideal transformer :  \\nb  \\n[  \\n]  \\n5  \\n[  \\n]  \\nidentity  \\nV  \\nP  \\nV  \\nS  \\n=  \\nI  \\nS  \\nI  \\nP  \\n=  \\nN  \\nP  \\nN  \\nS  \\n=  \\nL  \\nP  \\nL  \\nS  \\n=  \\na  \\n{\\\\displaystyle {\\\\frac {V_{\\\\text{P}}}{V_{\\\\text{S}}}}={\\\\frac {I_{\\\\text{S}}}{I_{\\\\text{P}}}}={\\\\frac {N_{\\\\text{P}}}{N_{\\\\text{S}}}}={\\\\sqrt {\\\\frac {L_{\\\\text{P}}}{L_{\\\\text{S}}}}}=a}  \\nEq. 5  \\nwhere is the primary winding and is the secondary winding self-inductance.  \\nL  \\nP  \\n{\\\\displaystyle L_{\\\\text{P}}}  \\nself-inductance  \\nL  \\nS  \\n{\\\\displaystyle L_{\\\\text{S}}}  \\nBy and the ideal transformer identity:  \\nOhm\\'s law  \\nZ  \\nL  \\n=  \\nV  \\nS  \\nI  \\nS  \\n{\\\\displaystyle Z_{\\\\text{L}}={\\\\frac {V_{\\\\text{S}}}{I_{\\\\text{S}}}}}  \\nEq. 6  \\nwhere is the load impedance of the secondary circuit and is the apparent load or driving point impedance of the primary circuit, the superscript denoting impedance referred to the primary.  \\nZ  \\nL  \\n′  \\n=  \\nV  \\nP  \\nI  \\nP  \\n=  \\na  \\nV  \\nS  \\nI  \\nS  \\n/  \\na  \\n=  \\na  \\n2  \\nV  \\nS  \\nI  \\nS  \\n=  \\na  \\n2  \\nZ  \\nL  \\n{\\\\displaystyle Z\\'_{\\\\text{L}}={\\\\frac {V_{\\\\text{P}}}{I_{\\\\text{P}}}}={\\\\frac {aV_{\\\\text{S}}}{I_{\\\\text{S}}/a}}=a^{2}{\\\\frac {V_{\\\\text{S}}}{I_{\\\\text{S}}}}=a^{2}{Z_{\\\\text{L}}}}  \\nEq. 7  \\nZ  \\nL  \\n{\\\\displaystyle Z_{\\\\text{L}}}  \\nZ  \\nL  \\n′  \\n{\\\\displaystyle Z\\'_{\\\\text{L}}}  \\n′  \\n{\\\\displaystyle \\'}'),\n",
       " Document(metadata={'Header3': 'Ideal transformer'}, page_content='Ideal transformer'),\n",
       " Document(metadata={}, page_content=\"[  \\nedit  \\n]  \\nAn ideal transformer is , lossless and perfectly . Perfect coupling implies infinitely high core and winding and zero net (i.e. − =\\xa00).  \\nlinear  \\ncoupled  \\nmagnetic permeability  \\ninductance  \\nmagnetomotive force  \\ni  \\np  \\nn  \\np  \\ni  \\ns  \\nn  \\ns  \\n4  \\n[  \\n]  \\nc  \\n[  \\n]  \\nIdeal transformer connected with source on primary and load impedance on secondary, where 0\\xa0< <\\xa0∞.  \\nV  \\nP  \\nZ  \\nL  \\nZ  \\nL  \\nIdeal transformer and induction law  \\nd  \\n[  \\n]  \\nA varying current in the transformer's primary winding creates a varying magnetic flux in the transformer core, which is also encircled by the secondary winding. This varying flux at the secondary winding induces a varying in the secondary winding. This electromagnetic induction phenomenon is the basis of transformer action and, in accordance with , the secondary current so produced creates a flux equal and opposite to that produced by the primary winding.  \\nelectromotive force or voltage  \\nLenz's law  \\nThe windings are wound around a core of infinitely high magnetic permeability so that all of the magnetic flux passes through both the primary and secondary windings. With a connected to the primary winding and a load connected to the secondary winding, the transformer currents flow in the indicated directions and the core magnetomotive force cancels to zero.  \\nvoltage source  \\nAccording to , since the same magnetic flux passes through both the primary and secondary windings in an ideal transformer, a voltage is induced in each winding proportional to its number of turns. The transformer winding voltage ratio is equal to the winding turns ratio.  \\nFaraday's law  \\n7  \\n[  \\n]  \\nAn ideal transformer is a reasonable approximation for a typical commercial transformer, with voltage ratio and winding turns ratio both being inversely proportional to the corresponding current ratio.  \\nThe load impedance to the primary circuit is equal to the turns ratio squared times the secondary circuit load impedance.  \\nreferred  \\n8  \\n[  \\n]\"),\n",
       " Document(metadata={'Header3': 'Real transformer'}, page_content='Real transformer'),\n",
       " Document(metadata={'Header3': 'Real transformer'}, page_content='[  \\nedit  \\n]  \\nLeakage flux of a transformer'),\n",
       " Document(metadata={'Header3': 'Real transformer', 'Header4': 'Deviations from ideal transformer'}, page_content='Deviations from ideal transformer'),\n",
       " Document(metadata={}, page_content=\"[  \\nedit  \\n]  \\nThe ideal transformer model neglects many basic linear aspects of real transformers, including unavoidable losses and inefficiencies.  \\n9  \\n[  \\n]  \\n(a) Core losses, collectively called magnetizing current losses, consisting of  \\n10  \\n[  \\n]  \\nlosses due to nonlinear magnetic effects in the transformer core, and  \\nHysteresis  \\nlosses due to joule heating in the core that are proportional to the square of the transformer's applied voltage.  \\nEddy current  \\n(b) Unlike the ideal model, the windings in a real transformer have non-zero resistances and inductances associated with:  \\ndue to resistance in the primary and secondary windings  \\nJoule losses  \\n10  \\n[  \\n]  \\nLeakage flux that escapes from the core and passes through one winding only resulting in primary and secondary reactive impedance.  \\n(c) similar to an , parasitic capacitance and self-resonance phenomenon due to the electric field distribution. Three kinds of parasitic capacitance are usually considered and the closed-loop equations are provided  \\ninductor  \\n11  \\n[  \\n]  \\nCapacitance between adjacent turns in any one layer;  \\nCapacitance between adjacent layers;  \\nCapacitance between the core and the layer(s) adjacent to the core;  \\nInclusion of capacitance into the transformer model is complicated, and is rarely attempted; the does not include parasitic capacitance. However, the capacitance effect can be measured by comparing open-circuit inductance, i.e. the inductance of a primary winding when the secondary circuit is open, to a short-circuit inductance when the secondary winding is shorted.  \\n'real' transformer model's equivalent circuit shown below\"),\n",
       " Document(metadata={'Header4': 'Leakage flux'}, page_content='Leakage flux'),\n",
       " Document(metadata={'Header4': 'Leakage flux'}, page_content='[  \\nedit  \\n]  \\nMain article:  \\nLeakage inductance  \\nThe ideal transformer model assumes that all flux generated by the primary winding links all the turns of every winding, including itself. In practice, some flux traverses paths that take it outside the windings. Such flux is termed , and results in in with the mutually coupled transformer windings. Leakage flux results in energy being alternately stored in and discharged from the magnetic fields with each cycle of the power supply. It is not directly a power loss, but results in inferior , causing the secondary voltage not to be directly proportional to the primary voltage, particularly under heavy load. Transformers are therefore normally designed to have very low leakage inductance.  \\n12  \\n[  \\n]  \\nleakage flux  \\nleakage inductance  \\nseries  \\n13  \\n[  \\n]  \\nvoltage regulation  \\n12  \\n[  \\n]  \\nIn some applications increased leakage is desired, and long magnetic paths, air gaps, or magnetic bypass shunts may deliberately be introduced in a transformer design to limit the current it will supply. Leaky transformers may be used to supply loads that exhibit , such as , and vapor lamps and or for safely handling loads that become periodically short-circuited such as .  \\nshort-circuit  \\n13  \\n[  \\n]  \\nnegative resistance  \\nelectric arcs  \\nmercury-  \\nsodium-  \\nneon signs  \\nelectric arc welders  \\n10  \\n[  \\n]  \\n:\\u200a485  \\nare also used to keep a transformer from saturating, especially audio-frequency transformers in circuits that have a DC component flowing in the windings. A exploits saturation of the core to control alternating current.  \\nAir gaps  \\n14  \\n[  \\n]  \\nsaturable reactor  \\nKnowledge of leakage inductance is also useful when transformers are operated in parallel. It can be shown that if the and associated winding leakage reactance-to-resistance ( / ) ratio of two transformers were\\nthe same, the transformers would share the load power in proportion to their respective ratings.  However, the impedance tolerances of commercial transformers are significant. Also, the impedance and X/R ratio of different capacity transformers tends to vary.  \\npercent impedance  \\ne  \\n[  \\n]  \\nX  \\nR  \\n16  \\n[  \\n]'),\n",
       " Document(metadata={'Header4': 'Equivalent circuit'}, page_content='Equivalent circuit'),\n",
       " Document(metadata={'Header4': 'Equivalent circuit'}, page_content='[  \\nedit  \\n]  \\nSee also:  \\nSteinmetz equivalent circuit  \\nReferring to the diagram, a practical transformer\\'s physical behavior may be represented by an model, which can incorporate an ideal transformer.  \\nequivalent circuit  \\n17  \\n[  \\n]  \\nWinding joule losses and leakage reactance are represented by the following series loop impedances of the model:  \\nPrimary winding: ,  \\nR  \\nP  \\nX  \\nP  \\nSecondary winding: , .  \\nR  \\nS  \\nX  \\nS  \\nIn normal course of circuit equivalence transformation, and are in practice usually referred to the primary side by multiplying these impedances by the turns ratio squared, ( / ) =\\xa0a .  \\nR  \\nS  \\nX  \\nS  \\nN  \\nP  \\nN  \\nS  \\n2  \\n2  \\nReal transformer equivalent circuit  \\nCore loss and reactance is represented by the following shunt leg impedances of the model:  \\nCore or iron losses:  \\nR  \\nC  \\nMagnetizing reactance: .  \\nX  \\nM  \\nand are collectively termed the of the model.  \\nR  \\nC  \\nX  \\nM  \\nmagnetizing branch  \\nCore losses are caused mostly by hysteresis and eddy current effects in the core and are proportional to the square of the core flux for operation at a given frequency. The finite permeability core requires a magnetizing current to maintain mutual flux in the core. Magnetizing current is in phase with the flux, the relationship between the two being non-linear due to saturation effects. However, all impedances of the equivalent circuit shown are by definition linear and such non-linearity effects are not typically reflected in transformer equivalent circuits. With supply, core flux lags the induced EMF by\\xa090°. With open-circuited secondary winding, magnetizing branch current equals transformer no-load current.  \\n10  \\n[  \\n]  \\n:\\u200a142–143  \\nI  \\nM  \\n10  \\n[  \\n]  \\n:\\u200a142  \\nsinusoidal  \\nI  \\n0  \\n17  \\n[  \\n]  \\nInstrument transformer, with and X1 markings on low-voltage (\"LV\") side terminal  \\npolarity dot  \\nThe resulting model, though sometimes termed \\'exact\\' equivalent circuit based on assumptions, retains a number of approximations. Analysis may be simplified by assuming that magnetizing branch impedance is relatively high and relocating the branch to the left of the primary impedances. This introduces error but allows combination of primary and referred secondary resistances and reactance by simple summation as two series impedances.  \\nlinearity  \\n17  \\n[  \\n]  \\nTransformer equivalent circuit impedance and transformer ratio parameters can be derived from the following tests: , , winding resistance test, and transformer ratio test.  \\nopen-circuit test  \\nshort-circuit test'),\n",
       " Document(metadata={'Header3': 'Transformer EMF equation'}, page_content='Transformer EMF equation'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\nIf the flux in the core is purely , the relationship for either winding between its of the winding, and the supply frequency , number of turns , core cross-sectional area in m and peak magnetic flux density in Wb/m or T (tesla) is given by the universal EMF equation:  \\nsinusoidal  \\nvoltage  \\nrms  \\nE  \\nrms  \\nf  \\nN  \\nA  \\n2  \\nB  \\npeak  \\n2  \\n10  \\n[  \\n]  \\nE  \\nrms  \\n=  \\n2  \\nπ π  \\nf  \\nN  \\nA  \\nB  \\npeak  \\n2  \\n≈ ≈  \\n4.44  \\nf  \\nN  \\nA  \\nB  \\npeak  \\n{\\\\displaystyle E_{\\\\text{rms}}={\\\\frac {2\\\\pi fNAB_{\\\\text{peak}}}{\\\\sqrt {2}}}\\\\approx 4.44fNAB_{\\\\text{peak}}}'),\n",
       " Document(metadata={'Header3': 'Polarity'}, page_content='Polarity'),\n",
       " Document(metadata={}, page_content=\"[  \\nedit  \\n]  \\nA is often used in transformer circuit diagrams, nameplates or terminal markings to define the relative polarity of transformer windings. Positively increasing instantaneous current entering the primary winding's 'dot' end induces positive polarity voltage exiting the secondary winding's 'dot' end. Three-phase transformers used in electric power systems will have a nameplate that indicate the phase relationships between their terminals. This may be in the form of a diagram, or using an alpha-numeric code to show the type of internal connection (wye or delta) for each winding.  \\ndot convention  \\nphasor\"),\n",
       " Document(metadata={'Header3': 'Effect of frequency'}, page_content='Effect of frequency'),\n",
       " Document(metadata={}, page_content=\"[  \\nedit  \\n]  \\nThe EMF of a transformer at a given flux increases with frequency. By operating at higher frequencies, transformers can be physically more compact because a given core is able to transfer more power without reaching saturation and fewer turns are needed to achieve the same impedance. However, properties such as core loss and conductor also increase with frequency. Aircraft and military equipment employ 400\\xa0Hz power supplies which reduce core and winding weight. Conversely, frequencies used for some were much lower (e.g. 16.7\\xa0Hz and 25\\xa0Hz) than normal utility frequencies (50–60\\xa0Hz) for historical reasons concerned mainly with the limitations of early . Consequently, the transformers used to step-down the high overhead line voltages were much larger and heavier for the same power rating than those required for the higher frequencies.  \\n10  \\n[  \\n]  \\nskin effect  \\n18  \\n[  \\n]  \\nrailway electrification systems  \\nelectric traction motors  \\nPower transformer overexcitation condition caused by decreased frequency; flux (green), iron core's magnetic characteristics (red) and magnetizing current (blue).  \\nOperation of a transformer at its designed voltage but at a higher frequency than intended will lead to reduced magnetizing current. At a lower frequency, the magnetizing current will increase. Operation of a large transformer at other than its design frequency may require assessment of voltages, losses, and cooling to establish if safe operation is practical. Transformers may require to protect the transformer from overvoltage at higher than rated frequency.  \\nprotective relays  \\nOne example is in traction transformers used for and train service operating across regions with different electrical standards. The converter equipment and traction transformers have to accommodate different input frequencies and voltage (ranging from as high as 50\\xa0Hz down to 16.7\\xa0Hz and rated up to 25\\xa0kV).  \\nelectric multiple unit  \\nhigh-speed  \\nAt much higher frequencies the transformer core size required drops dramatically: a physically small transformer can handle power levels that would require a massive iron core at mains frequency. The development of switching power semiconductor devices made viable, to generate a high frequency, then change the voltage level with a small transformer.  \\nswitch-mode power supplies  \\nTransformers for higher frequency applications such as typically use core materials with much lower hysteresis and eddy-current losses than those for 50/60\\xa0Hz. Primary examples are iron-powder and ferrite cores. The lower frequency-dependant losses of these cores often is at the expense of flux density at saturation. For instance, saturation occurs at a substantially lower flux density than laminated iron.  \\nSMPS  \\nferrite  \\nLarge power transformers are vulnerable to insulation failure due to transient voltages with high-frequency components, such as caused in switching or by lightning.\"),\n",
       " Document(metadata={'Header3': 'Energy losses'}, page_content='Energy losses'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\nTransformer energy losses are dominated by winding and core losses. Transformers\\' efficiency tends to improve with increasing transformer capacity. The efficiency of typical distribution transformers is between about 98 and 99 percent.  \\n19  \\n[  \\n]  \\n19  \\n[  \\n]  \\n20  \\n[  \\n]  \\nAs transformer losses vary with load, it is often useful to tabulate , full-load loss, half-load loss, and so on. Hysteresis and losses are constant at all load levels and dominate at no load, while winding loss increases as load increases. The no-load loss can be significant, so that even an idle transformer constitutes a drain on the electrical supply. Designing for lower loss requires a larger core, good-quality , or even for the core and thicker wire, increasing initial cost. The choice of construction represents a between initial cost and operating cost.  \\nno-load loss  \\neddy current  \\nenergy efficient transformers  \\nsilicon steel  \\namorphous steel  \\ntrade-off  \\n21  \\n[  \\n]  \\nTransformer losses arise from:  \\nWinding joule losses  \\nCurrent flowing through a winding\\'s conductor causes due to the of the wire. As frequency increases, skin effect and causes the winding\\'s resistance and, hence, losses to increase.  \\njoule heating  \\nresistance  \\nproximity effect  \\nCore losses  \\nHysteresis losses  \\nEach time the magnetic field is reversed, a small amount of energy is lost due to within the core, caused by motion of the within the steel. According to Steinmetz\\'s formula, the heat energy due to hysteresis is given by  \\nhysteresis  \\nmagnetic domains  \\nand,  \\nW  \\nh  \\n≈ ≈  \\nη η  \\nβ β  \\nmax  \\n1.6  \\n{\\\\displaystyle W_{\\\\text{h}}\\\\approx \\\\eta \\\\beta _{\\\\text{max}}^{1.6}}  \\nhysteresis loss is thus given by  \\nP  \\nh  \\n≈ ≈  \\nW  \\nh  \\nf  \\n≈ ≈  \\nη η  \\nf  \\nβ β  \\nmax  \\n1.6  \\n{\\\\displaystyle P_{\\\\text{h}}\\\\approx {W}_{\\\\text{h}}f\\\\approx \\\\eta {f}\\\\beta _{\\\\text{max}}^{1.6}}  \\nwhere, is the frequency, is the hysteresis coefficient and is the maximum flux density, the empirical exponent of which varies from about 1.4 to 1.8 but is often given as 1.6 for iron. For more detailed analysis, see and .  \\nf  \\nη  \\nβ  \\nmax  \\n21  \\n[  \\n]  \\nMagnetic core  \\nSteinmetz\\'s equation  \\nEddy current losses  \\nare induced in the conductive metal transformer core by the changing magnetic field, and this current flowing through the resistance of the iron dissipates energy as heat in the core. The eddy current loss is a complex function of the square of supply frequency and inverse square of the material thickness. Eddy current losses can be reduced by making the core of a stack of laminations (thin plates) electrically insulated from each other, rather than a solid block; all transformers operating at low frequencies use laminated or similar cores.  \\nEddy currents  \\n21  \\n[  \\n]  \\nMagnetostriction related transformer hum  \\nMagnetic flux in a ferromagnetic material, such as the core, causes it to physically expand and contract slightly with each cycle of the magnetic field, an effect known as , the frictional energy of which produces an audible noise known as or \"transformer hum\". This transformer hum is especially objectionable in transformers supplied at and in associated with television .  \\nmagnetostriction  \\nmains hum  \\n22  \\n[  \\n]  \\npower frequencies  \\nhigh-frequency  \\nflyback transformers  \\nCRTs  \\nStray losses  \\nLeakage inductance is by itself largely lossless, since energy supplied to its magnetic fields is returned to the supply with the next half-cycle. However, any leakage flux that intercepts nearby conductive materials such as the transformer\\'s support structure will give rise to eddy currents and be converted to heat.  \\n23  \\n[  \\n]  \\nRadiative  \\nThere are also radiative losses due to the oscillating magnetic field but these are usually small.  \\nMechanical vibration and audible noise transmission  \\nIn addition to magnetostriction, the alternating magnetic field causes fluctuating forces between the primary and secondary windings. This energy incites vibration transmission in interconnected metalwork, thus amplifying audible transformer hum.  \\n24  \\n[  \\n]'),\n",
       " Document(metadata={'Header2': 'Construction'}, page_content='Construction'),\n",
       " Document(metadata={'Header2': 'Construction'}, page_content='[  \\nedit  \\n]'),\n",
       " Document(metadata={'Header2': 'Construction', 'Header3': 'Cores'}, page_content='Cores'),\n",
       " Document(metadata={}, page_content=\"[  \\nedit  \\n]  \\nCore form = core type; shell form = shell type  \\nClosed-core transformers are constructed in 'core form' or 'shell form'. When windings surround the core, the transformer is core form; when windings are surrounded by the core, the transformer is shell form. Shell form design may be more prevalent than core form design for distribution transformer applications due to the relative ease in stacking the core around winding coils. Core form design tends to, as a general rule, be more economical, and therefore more prevalent, than shell form design for high voltage power transformer applications at the lower end of their voltage and power rating ranges (less than or equal to, nominally, 230\\xa0kV or 75\\xa0MVA). At higher voltage and power ratings, shell form transformers tend to be more prevalent. Shell form design tends to be preferred for extra-high voltage and higher MVA applications because, though more labor-intensive to manufacture, shell form transformers are characterized as having inherently better kVA-to-weight ratio, better short-circuit strength characteristics and higher immunity to transit damage.  \\n25  \\n[  \\n]  \\n25  \\n[  \\n]  \\n25  \\n[  \\n]  \\n26  \\n[  \\n]  \\n27  \\n[  \\n]  \\n27  \\n[  \\n]\"),\n",
       " Document(metadata={'Header4': 'Laminated steel cores'}, page_content='Laminated steel cores'),\n",
       " Document(metadata={}, page_content=\"[  \\nedit  \\n]  \\nShell type transformer with laminated core showing edges of laminations at the top of the photo  \\nInterleaved E-I transformer laminations showing air gap and flux paths  \\nTransformers for use at power or audio frequencies typically have cores made of high permeability . The steel has a permeability many times that of and the core thus serves to greatly reduce the magnetizing current and confine the flux to a path which closely couples the windings. Early transformer developers soon realized that cores constructed from solid iron resulted in prohibitive eddy current losses, and their designs mitigated this effect with cores consisting of bundles of insulated iron wires. Later designs constructed the core by stacking layers of thin steel laminations, a principle that has remained in use. Each lamination is insulated from its neighbors by a thin non-conducting layer of insulation. The can be used to calculate the core cross-sectional area for a preferred level of magnetic flux.  \\nsilicon steel  \\n28  \\n[  \\n]  \\nfree space  \\n29  \\n[  \\n]  \\n30  \\n[  \\n]  \\n31  \\n[  \\n]  \\ntransformer universal EMF equation  \\n10  \\n[  \\n]  \\nThe effect of laminations is to confine to highly elliptical paths that enclose little flux, and so reduce their magnitude. Thinner laminations reduce losses, but are more laborious and expensive to construct. Thin laminations are generally used on high-frequency transformers, with some of very thin steel laminations able to operate up to 10\\xa0kHz.  \\neddy currents  \\n28  \\n[  \\n]  \\n32  \\n[  \\n]  \\nLaminating the core greatly reduces eddy-current losses  \\nOne common design of laminated core is made from interleaved stacks of steel sheets capped with pieces, leading to its name of . Such a design tends to exhibit more losses, but is very economical to manufacture. The cut-core or C-core type is made by winding a steel strip around a rectangular form and then bonding the layers together. It is then cut in two, forming two C shapes, and the core assembled by binding the two C halves together with a steel strap. They have the advantage that the flux is always oriented parallel to the metal grains, reducing reluctance.  \\nE-shaped  \\nI-shaped  \\nE-I transformer  \\n32  \\n[  \\n]  \\n32  \\n[  \\n]  \\nA steel core's means that it retains a static magnetic field when power is removed. When power is then reapplied, the residual field will cause a high until the effect of the remaining magnetism is reduced, usually after a few cycles of the applied AC waveform. Overcurrent protection devices such as must be selected to allow this harmless inrush to pass.  \\nremanence  \\ninrush current  \\n33  \\n[  \\n]  \\nfuses  \\nOn transformers connected to long, overhead power transmission lines, induced currents due to during can cause and operation of transformer protection devices.  \\ngeomagnetic disturbances  \\nsolar storms  \\nsaturation of the core  \\n34  \\n[  \\n]  \\nDistribution transformers can achieve low no-load losses by using cores made with low-loss high-permeability silicon steel or . The higher initial cost of the core material is offset over the life of the transformer by its lower losses at light load.  \\namorphous (non-crystalline) metal alloy  \\n35  \\n[  \\n]\"),\n",
       " Document(metadata={'Header4': 'Solid cores'}, page_content='Solid cores'),\n",
       " Document(metadata={}, page_content=\"[  \\nedit  \\n]  \\nPowdered iron cores are used in circuits such as switch-mode power supplies that operate above mains frequencies and up to a few tens of kilohertz. These materials combine high magnetic permeability with high bulk electrical . For frequencies extending beyond the , cores made from non-conductive magnetic materials called are common. Some radio-frequency transformers also have movable cores (sometimes called 'slugs') which allow adjustment of the (and ) of tuned radio-frequency circuits.  \\nresistivity  \\nVHF band  \\nceramic  \\nferrites  \\n32  \\n[  \\n]  \\ncoupling coefficient  \\nbandwidth\"),\n",
       " Document(metadata={'Header4': 'Toroidal cores'}, page_content='Toroidal cores'),\n",
       " Document(metadata={}, page_content=\"[  \\nedit  \\n]  \\nSmall toroidal core transformer  \\nToroidal transformers are built around a ring-shaped core, which, depending on operating frequency, is made from a long strip of or wound into a coil, powdered iron, or . A strip construction ensures that the are optimally aligned, improving the transformer's efficiency by reducing the core's . The closed ring shape eliminates air gaps inherent in the construction of an E-I core. The cross-section of the ring is usually square or rectangular, but more expensive cores with circular cross-sections are also available. The primary and secondary coils are often wound concentrically to cover the entire surface of the core. This minimizes the length of wire needed and provides screening to minimize the core's magnetic field from generating .  \\nsilicon steel  \\npermalloy  \\nferrite  \\n36  \\n[  \\n]  \\ngrain boundaries  \\nreluctance  \\n10  \\n[  \\n]  \\n:\\u200a485  \\nelectromagnetic interference  \\nToroidal transformers are more efficient than the cheaper laminated E-I types for a similar power level. Other advantages compared to E-I types, include smaller size (about half), lower weight (about half), less mechanical hum (making them superior in audio amplifiers), lower exterior magnetic field (about one tenth), low off-load losses (making them more efficient in standby circuits), single-bolt mounting, and greater choice of shapes. The main disadvantages are higher cost and limited power capacity (see below). Because of the lack of a residual gap in the magnetic path, toroidal transformers also tend to exhibit higher inrush current, compared to laminated E-I types.  \\nClassification parameters  \\nFerrite toroidal cores are used at higher frequencies, typically between a few tens of kilohertz to hundreds of megahertz, to reduce losses, physical size, and weight of inductive components. A drawback of toroidal transformer construction is the higher labor cost of winding. This is because it is necessary to pass the entire length of a coil winding through the core aperture each time a single turn is added to the coil. As a consequence, toroidal transformers rated more than a few kVA are uncommon. Relatively few toroids are offered with power ratings above 10\\xa0kVA, and practically none above 25\\xa0kVA. Small distribution transformers may achieve some of the benefits of a toroidal core by splitting it and forcing it open, then inserting a bobbin containing primary and secondary windings.  \\n37  \\n[  \\n]\"),\n",
       " Document(metadata={'Header4': 'Air cores'}, page_content='Air cores'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\nA transformer can be produced by placing the windings near each other, an arrangement termed an \"air-core\" transformer. An air-core transformer eliminates loss due to hysteresis in the core material. The magnetizing inductance is drastically reduced by the lack of a magnetic core, resulting in large magnetizing currents and losses if used at low frequencies. Air-core transformers are unsuitable for use in power distribution, but are frequently employed in radio-frequency applications. Air cores are also used for such as , where they can achieve reasonably low loss despite the low magnetizing inductance.  \\n13  \\n[  \\n]  \\n13  \\n[  \\n]  \\n38  \\n[  \\n]  \\nresonant transformers  \\ntesla coils'),\n",
       " Document(metadata={'Header3': 'Windings'}, page_content='Windings'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\nWindings are usually arranged concentrically to minimize flux leakage.  \\nCut view through transformer windings. Legend: : Air, liquid or other insulating medium : : Primary winding : Secondary winding  \\nWhite  \\nGreen spiral  \\nGrain oriented silicon steel  \\nBlack  \\nRed  \\nThe electrical conductor used for the windings depends upon the application, but in all cases the individual turns must be electrically insulated from each other to ensure that the current travels throughout every turn. For small transformers, in which currents are low and the potential difference between adjacent turns is small, the coils are often wound from . Larger power transformers may be wound with copper rectangular strip conductors insulated by oil-impregnated paper and blocks of .  \\nenameled magnet wire  \\npressboard  \\n39  \\n[  \\n]  \\nHigh-frequency transformers operating in the tens to hundreds of kilohertz often have windings made of braided to minimize the and proximity effect losses. Large power transformers use multiple-stranded conductors as well, since even at low power frequencies non-uniform distribution of current would otherwise exist in high-current windings. Each strand is individually insulated, and the strands are arranged so that at certain points in the winding, or throughout the whole winding, each portion occupies different relative positions in the complete conductor. The transposition equalizes the current flowing in each strand of the conductor, and reduces eddy current losses in the winding itself. The stranded conductor is also more flexible than a solid conductor of similar size, aiding manufacture.  \\nLitz wire  \\nskin-effect  \\n40  \\n[  \\n]  \\n39  \\n[  \\n]  \\n39  \\n[  \\n]  \\nThe windings of signal transformers minimize leakage inductance and stray capacitance to improve high-frequency response. Coils are split into sections, and those sections interleaved between the sections of the other winding.  \\nPower-frequency transformers may have at intermediate points on the winding, usually on the higher voltage winding side, for voltage adjustment. Taps may be manually reconnected, or a manual or automatic switch may be provided for changing taps. Automatic on-load are used in electric power transmission or distribution, on equipment such as transformers, or for automatic voltage regulators for sensitive loads. Audio-frequency transformers, used for the distribution of audio to public address loudspeakers, have taps to allow adjustment of impedance to each speaker. A is often used in the output stage of an audio power in a . Modulation transformers in transmitters are very similar.  \\ntaps  \\ntap changers  \\narc furnace  \\ncenter-tapped transformer  \\namplifier  \\npush-pull circuit  \\nAM'),\n",
       " Document(metadata={'Header3': 'Cooling'}, page_content='Cooling'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\nCutaway view of liquid-immersed transformer. The conservator (reservoir) at top provides liquid-to-atmosphere isolation as coolant level and temperature changes. The walls and fins provide required heat dissipation.  \\nIt is a rule of thumb that the life expectancy of electrical insulation is halved for about every 7\\xa0°C to 10\\xa0°C increase in (an instance of the application of the ).  \\noperating temperature  \\nArrhenius equation  \\n41  \\n[  \\n]  \\nSmall dry-type and liquid-immersed transformers are often self-cooled by natural convection and heat dissipation. As power ratings increase, transformers are often cooled by forced-air cooling, forced-oil cooling, water-cooling, or combinations of these. Large transformers are filled with that both cools and insulates the windings. Transformer oil is often a highly refined that cools the windings and insulation by circulating within the transformer tank. The mineral oil and insulation system has been extensively studied and used for more than 100 years. It is estimated that 50% of power transformers will survive 50 years of use, that the average age of failure of power transformers is about 10 to 15 years, and that about 30% of power transformer failures are due to insulation and overloading failures. Prolonged operation at elevated temperature degrades insulating properties of winding insulation and dielectric coolant, which not only shortens transformer life but can ultimately lead to catastrophic transformer failure. With a great body of empirical study as a guide, including provides valuable maintenance information.  \\nradiation  \\n42  \\n[  \\n]  \\ntransformer oil  \\n43  \\n[  \\n]  \\nmineral oil  \\npaper  \\n44  \\n[  \\n]  \\n45  \\n[  \\n]  \\n41  \\n[  \\n]  \\ntransformer oil testing  \\ndissolved gas analysis  \\nBuilding regulations in many jurisdictions require indoor liquid-filled transformers to either use dielectric fluids that are less flammable than oil, or be installed in fire-resistant rooms. Air-cooled dry transformers can be more economical where they eliminate the cost of a fire-resistant transformer room.  \\n19  \\n[  \\n]  \\nThe tank of liquid-filled transformers often has radiators through which the liquid coolant circulates by natural convection or fins. Some large transformers employ electric fans for forced-air cooling, pumps for forced-liquid cooling, or have for water-cooling. An oil-immersed transformer may be equipped with a , which, depending on severity of gas accumulation due to internal arcing, is used to either trigger an alarm or de-energize the transformer. Oil-immersed transformer installations usually include fire protection measures such as walls, oil containment, and fire-suppression sprinkler systems.  \\nheat exchangers  \\n43  \\n[  \\n]  \\nBuchholz relay  \\n33  \\n[  \\n]  \\n(PCBs) have properties that once favored their use as a , though concerns over their led to a widespread ban on their use. Today, non-toxic, stable -based oils, or may be used where the expense of a fire-resistant liquid offsets additional building cost for a transformer vault. However, the long life span of transformers can mean that the potential for exposure can be high long after banning.  \\nPolychlorinated biphenyls  \\ndielectric coolant  \\nenvironmental persistence  \\n46  \\n[  \\n]  \\nsilicone  \\nfluorinated hydrocarbons  \\n19  \\n[  \\n]  \\n47  \\n[  \\n]  \\n48  \\n[  \\n]  \\nSome transformers are gas-insulated.  Their windings are enclosed in sealed, pressurized tanks and often cooled by or gas.  \\nnitrogen  \\nsulfur hexafluoride  \\n47  \\n[  \\n]  \\nExperimental power transformers in the 500–1,000\\xa0kVA range have been built with or cooled windings, which eliminates winding losses without affecting core losses.  \\nliquid nitrogen  \\nhelium  \\nsuperconducting  \\n49  \\n[  \\n]  \\n50  \\n[  \\n]'),\n",
       " Document(metadata={'Header3': 'Insulation'}, page_content='Insulation'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\nSubstation transformer undergoing testing.  \\nInsulation must be provided between the individual turns of the windings, between the windings, between windings and core, and at the terminals of the winding.  \\nInter-turn insulation of small transformers may be a layer of insulating varnish on the wire. Layer of paper or polymer films may be inserted between layers of windings, and between primary and secondary windings. A transformer may be coated or dipped in a polymer resin to improve the strength of windings and protect them from moisture or corrosion. The resin may be impregnated into the winding insulation using combinations of vacuum and pressure during the coating process, eliminating all air voids in the winding. In the limit, the entire coil may be placed in a mold, and resin cast around it as a solid block, encapsulating the windings.  \\n51  \\n[  \\n]  \\nLarge oil-filled power transformers use windings wrapped with insulating paper, which is impregnated with oil during assembly of the transformer. Oil-filled transformers use highly refined mineral oil to insulate and cool the windings and core. \\nConstruction of oil-filled transformers requires that the insulation covering the windings be thoroughly dried of residual moisture before the oil is introduced. Drying may be done by circulating hot air around the core, by circulating externally heated transformer oil, or by vapor-phase drying (VPD) where an evaporated solvent transfers heat by condensation on the coil and core. For small transformers, resistance heating by injection of current into the windings is used.'),\n",
       " Document(metadata={'Header3': 'Bushings'}, page_content='Bushings'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\nLarger transformers are provided with high-voltage insulated made of polymers or porcelain. A large bushing can be a complex structure since it must provide careful control of the without letting the transformer leak oil.  \\nbushings  \\nelectric field gradient  \\n52  \\n[  \\n]'),\n",
       " Document(metadata={'Header2': 'Classification parameters'}, page_content='Classification parameters'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\nAn in , showing three of five 220\\xa0kV – 66\\xa0kV transformers, each with a capacity of 150\\xa0MVA  \\nelectrical substation  \\nMelbourne  \\nAustralia  \\ntransformer in ,  \\nCamouflaged  \\nLangley City  \\nCanada  \\nTransformers can be classified in many ways, such as the following:  \\n: From a fraction of a volt-ampere (VA) to over a thousand MVA.  \\nPower rating  \\n: Continuous, short-time, intermittent, periodic, varying.  \\nDuty of a transformer  \\n: , , or .  \\nFrequency range  \\nPower-frequency  \\naudio-frequency  \\nradio-frequency  \\n: From a few volts to hundreds of kilovolts.  \\nVoltage class  \\n: Dry or liquid-immersed;  self-cooled, forced air-cooled;forced oil-cooled, water-cooled.  \\nCooling type  \\n:  power supply, impedance matching, output voltage and current stabilizer, , circuit isolation, , , , amplifier output, etc..  \\nApplication  \\npulse  \\npower distribution  \\nrectifier  \\narc furnace  \\n: Core form, shell form, concentric, sandwich.  \\nBasic magnetic form  \\n:  Step-up,  step-down, .  \\nConstant-potential transformer descriptor  \\nisolation  \\n: By , two-winding combinations of the phase designations  delta, wye or star, and ; ,  \\nGeneral winding configuration  \\nIEC vector group  \\nzigzag  \\nautotransformer  \\nScott-T  \\n: 2-winding, 6-pulse; 3-winding, 12-pulse; . . ., , [ −\\xa01]·6-pulse; polygon; etc.  \\nRectifier phase-shift winding configuration  \\n-winding  \\nn  \\nn  \\n: A measure of how well the transformer can withstand harmonic loads.  \\nK-factor  \\n53  \\n[  \\n]'),\n",
       " Document(metadata={'Header2': 'Applications'}, page_content='Applications'),\n",
       " Document(metadata={'Header2': 'Applications'}, page_content=\"[  \\nedit  \\n]  \\nTransformer at the in , Canada  \\nLimestone Generating Station  \\nManitoba  \\nMain article:  \\nTransformer types  \\nVarious specific electrical application designs require a variety of . Although they all share the basic characteristic transformer principles, they are customized in construction or electrical properties for certain installation requirements or circuit conditions.  \\ntransformer types  \\nIn , transformers allow transmission of electric power at high voltages, which reduces the loss due to heating of the wires. This allows generating plants to be located economically at a distance from electrical consumers. All but a tiny fraction of the world's electrical power has passed through a series of transformers by the time it reaches the consumer.  \\nelectric power transmission  \\n54  \\n[  \\n]  \\n23  \\n[  \\n]  \\nIn many electronic devices, a transformer is used to convert voltage from the distribution wiring to convenient values for the circuit requirements, either directly at the power line frequency or through a .  \\nswitch mode power supply  \\nSignal and audio transformers are used to couple stages of and to match devices such as and to the input of amplifiers. Audio transformers allowed circuits to carry on a over a single pair of wires. A transformer converts a signal that is referenced to ground to a signal that has , such as between external cables and internal circuits. Isolation transformers prevent leakage of current into the secondary circuit and are used in medical equipment and at construction sites. Resonant transformers are used for coupling between stages of radio receivers, or in high-voltage Tesla coils.  \\namplifiers  \\nmicrophones  \\nrecord players  \\ntelephone  \\ntwo-way conversation  \\nbalun  \\nbalanced voltages to ground  \\nSchematic of a large oil-filled power transformer  \\n1. Tank  \\n2. Lid  \\n3. Conservator tank  \\n4. Oil level indicator  \\n5. Buchholz relay for detecting gas bubbles after an internal fault  \\n6. Piping  \\n7. Tap changer  \\n8. Drive motor for tap changer  \\n9. Drive shaft for tap changer  \\n10. High voltage (HV) bushing  \\n11. High voltage bushing current transformers  \\n12. Low voltage (LV) bushing  \\n13. Low voltage current transformers  \\n14. Bushing voltage-transformer for metering  \\n15. Core  \\n16. Yoke of the core  \\n17. Limbs connect the yokes and hold them up  \\n18. Coils  \\n19. Internal wiring between coils and tapchanger  \\n20. Oil release valve  \\n21. Vacuum valve\"),\n",
       " Document(metadata={'Header2': 'History'}, page_content='History'),\n",
       " Document(metadata={'Header2': 'History'}, page_content='[  \\nedit  \\n]'),\n",
       " Document(metadata={'Header2': 'History', 'Header3': 'Discovery of induction'}, page_content='Discovery of induction'),\n",
       " Document(metadata={}, page_content=\"[  \\nedit  \\n]  \\nFaraday's experiment with induction between coils of wire  \\n55  \\n[  \\n]  \\n, the principle of the operation of the transformer, was discovered independently by in 1831 and in 1832. Only Faraday furthered his experiments to the point of working out the equation describing the relationship between EMF and magnetic flux now known as :  \\nElectromagnetic induction  \\nMichael Faraday  \\nJoseph Henry  \\n56  \\n[  \\n]  \\n57  \\n[  \\n]  \\n58  \\n[  \\n]  \\n59  \\n[  \\n]  \\nFaraday's law of induction  \\n|  \\nE  \\n|  \\n=  \\n|  \\nd  \\nΦ Φ  \\nB  \\nd  \\nt  \\n|  \\n,  \\n{\\\\displaystyle |{\\\\mathcal {E}}|=\\\\left|{{\\\\mathrm {d} \\\\Phi _{\\\\text{B}}} \\\\over \\\\mathrm {d} t}\\\\right|,}  \\nwhere is the magnitude of the EMF in volts and Φ is the magnetic flux through the circuit in .  \\n|  \\nE  \\n|  \\n{\\\\displaystyle |{\\\\mathcal {E}}|}  \\nB  \\nwebers  \\n60  \\n[  \\n]  \\nFaraday performed early experiments on induction between coils of wire, including winding a pair of coils around an iron ring, thus creating the first closed-core transformer. However he only applied individual pulses of current to his transformer, and never discovered the relation between the turns ratio and EMF in the windings.  \\ntoroidal  \\n59  \\n[  \\n]  \\n61  \\n[  \\n]  \\nInduction coil, 1900, Bremerhaven, Germany\"),\n",
       " Document(metadata={'Header3': 'Induction coils'}, page_content='Induction coils'),\n",
       " Document(metadata={'Header3': 'Induction coils'}, page_content=\"[  \\nedit  \\n]  \\nMain article:  \\nInduction coil  \\nFaraday's ring transformer  \\nThe first type of transformer to see wide use was the , invented by Irish-Catholic Rev. of , Ireland in 1836. He was one of the first researchers to realize the more turns the secondary winding has in relation to the primary winding, the larger the induced secondary EMF will be. Induction coils evolved from scientists' and inventors' efforts to get higher voltages from batteries. Since batteries produce rather than AC, induction coils relied upon vibrating that regularly interrupted the current in the primary to create the flux changes necessary for induction. Between the 1830s and the 1870s, efforts to build better induction coils, mostly by trial and error, slowly revealed the basic principles of transformers.  \\ninduction coil  \\nNicholas Callan  \\nMaynooth College  \\n59  \\n[  \\n]  \\ndirect current (DC)  \\nelectrical contacts\"),\n",
       " Document(metadata={'Header3': 'First alternating current transformers'}, page_content='First alternating current transformers'),\n",
       " Document(metadata={}, page_content=\"[  \\nedit  \\n]  \\nBy the 1870s, efficient producing were available, and it was found AC could power an induction coil directly, without an .  \\ngenerators  \\nalternating current (AC)  \\ninterrupter  \\nIn 1876, Russian engineer invented a lighting system based on a set of induction coils where the primary windings were connected to a source of AC. The secondary windings could be connected to several (arc lamps) of his own design. The coils Yablochkov employed functioned essentially as transformers.  \\nPavel Yablochkov  \\n'electric candles'  \\n62  \\n[  \\n]  \\nIn 1878, the , Budapest, Hungary, began producing equipment for electric lighting and, by 1883, had installed over fifty systems in Austria-Hungary. Their AC systems used arc and incandescent lamps, generators, and other equipment.  \\nGanz factory  \\n59  \\n[  \\n]  \\n63  \\n[  \\n]  \\nIn 1882, and first exhibited a device with an initially widely criticized laminated plate open iron core called a 'secondary generator' in London, then sold the idea to the company in the United States in 1886. They also exhibited the invention in Turin, Italy in 1884, where it was highly successful and adopted for an electric lighting system. Their open-core device used a fixed 1:1 ratio to supply a series circuit for the utilization load (lamps). However, the voltage of their system was controlled by moving the iron core in or out.  \\nLucien Gaulard  \\nJohn Dixon Gibbs  \\nWestinghouse  \\n30  \\n[  \\n]  \\n64  \\n[  \\n]  \\n65  \\n[  \\n]\"),\n",
       " Document(metadata={'Header4': 'Early series circuit transformer distribution'}, page_content='Early series circuit transformer distribution'),\n",
       " Document(metadata={}, page_content=\"[  \\nedit  \\n]  \\nInduction coils with open magnetic circuits are inefficient at transferring power to . Until about 1880, the paradigm for AC power transmission from a high voltage supply to a low voltage load was a series circuit. Open-core transformers with a ratio near 1:1 were connected with their primaries in series to allow use of a high voltage for transmission while presenting a low voltage to the lamps. The inherent flaw in this method was that turning off a single lamp (or other electric device) affected the voltage supplied to all others on the same circuit. Many adjustable transformer designs were introduced to compensate for this problematic characteristic of the series circuit, including those employing methods of adjusting the core or bypassing the magnetic flux around part of a coil. Efficient, practical transformer designs did not appear until the 1880s, but within a decade, the transformer would be instrumental in the , and in seeing AC distribution systems triumph over their DC counterparts, a position in which they have remained dominant ever since.  \\nloads  \\n64  \\n[  \\n]  \\nwar of the currents  \\n66  \\n[  \\n]  \\nShell form transformer. Sketch used by Uppenborn to describe ZBD engineers' 1885 patents and earliest articles.  \\n64  \\n[  \\n]  \\nCore form, front; shell form, back. Earliest specimens of ZBD-designed high-efficiency constant-potential transformers manufactured at the Ganz factory in 1885.  \\nThe ZBD team consisted of , and  \\nKároly Zipernowsky  \\nOttó Bláthy  \\nMiksa Déri  \\nStanley's 1886 design for adjustable gap open-core induction coils\"),\n",
       " Document(metadata={'Header3': 'Closed-core transformers and parallel power distribution'}, page_content='Closed-core transformers and parallel power distribution'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\nIn the autumn of 1884, , and (ZBD), three Hungarian engineers associated with the , had determined that open-core devices were impracticable, as they were incapable of reliably regulating voltage. The Ganz factory had also in the autumn of 1884 made delivery of the world\\'s first five high-efficiency AC transformers, the first of these units having been shipped on September 16, 1884. This first unit had been manufactured to the following specifications: 1,400 W, 40\\xa0Hz, 120:72 V, 11.6:19.4 A, ratio 1.67:1, one-phase, shell form. In their joint 1885 patent applications for novel transformers (later called ZBD transformers), they described two designs with closed magnetic circuits where copper windings were either wound around an iron wire ring core or surrounded by an iron wire core. The two designs were the first application of the two basic transformer constructions in common use to this day, termed \"core form\" or \"shell form\" .  \\nKároly Zipernowsky  \\nOttó Bláthy  \\nMiksa Déri  \\nGanz Works  \\n63  \\n[  \\n]  \\n67  \\n[  \\n]  \\n67  \\n[  \\n]  \\n64  \\n[  \\n]  \\n68  \\n[  \\n]  \\nIn both designs, the magnetic flux linking the primary and secondary windings traveled almost entirely within the confines of the iron core, with no intentional path through air (see below). The new transformers were 3.4 times more efficient than the open-core bipolar devices of Gaulard and Gibbs. The ZBD patents included two other major interrelated innovations: one concerning the use of parallel connected, instead of series connected, utilization loads, the other concerning the ability to have high turns ratio transformers such that the supply network voltage could be much higher (initially 1,400 to 2,000 V) than the voltage of utilization loads (100 V initially preferred). When employed in parallel connected electric distribution systems, closed-core transformers finally made it technically and economically feasible to provide electric power for lighting in homes, businesses and public spaces. Bláthy had suggested the use of closed cores, Zipernowsky had suggested the use of , and Déri had performed the experiments; In early 1885, the three engineers also eliminated the problem of losses with the invention of the lamination of electromagnetic cores.  \\nToroidal cores  \\n69  \\n[  \\n]  \\n70  \\n[  \\n]  \\n71  \\n[  \\n]  \\nparallel shunt connections  \\n72  \\n[  \\n]  \\neddy current  \\n73  \\n[  \\n]  \\nTransformers today are designed on the principles discovered by the three engineers. They also popularized the word \\'transformer\\' to describe a device for altering the EMF of an electric current although the term had already been in use by 1882. In 1886, the ZBD engineers designed, and the Ganz factory supplied electrical equipment for, the world\\'s first that used AC generators to power a parallel connected common electrical network, the steam-powered Rome-Cerchi power plant.  \\n74  \\n[  \\n]  \\n75  \\n[  \\n]  \\n76  \\n[  \\n]  \\npower station  \\n77  \\n[  \\n]'),\n",
       " Document(metadata={'Header3': 'Westinghouse improvements'}, page_content='Westinghouse improvements'),\n",
       " Document(metadata={}, page_content=\"[  \\nedit  \\n]  \\nE-shaped plates for transformer cores developed by Westinghouse  \\nBuilding on the advancement of AC technology in Europe, founded the in Pittsburgh, Pennsylvania, on January 8, 1886. The new firm became active in developing alternating current (AC) electric infrastructure throughout the United States.  \\nThe held an option on the US rights for the ZBD transformers, requiring Westinghouse to pursue alternative designs on the same principles. George Westinghouse had bought Gaulard and Gibbs' patents for $50,000 in February 1886. He assigned to the task of redesign the Gaulard and Gibbs transformer for commercial use in United States. Stanley's first patented design was for induction coils with single cores of soft iron and adjustable gaps to regulate the EMF present in the secondary winding (see image). This design was first used commercially in the US in 1886 but Westinghouse was intent on improving the Stanley design to make it (unlike the ZBD type) easy and cheap to produce.  \\n78  \\n[  \\n]  \\nGeorge Westinghouse  \\nWestinghouse Electric  \\n79  \\n[  \\n]  \\nEdison Electric Light Company  \\n80  \\n[  \\n]  \\nWilliam Stanley  \\n81  \\n[  \\n]  \\n82  \\n[  \\n]  \\n83  \\n[  \\n]  \\n82  \\n[  \\n]  \\nWestinghouse, Stanley and associates soon developed a core that was easier to manufacture, consisting of a stack of thin 'E‑shaped' iron plates insulated by thin sheets of paper or other insulating material. Pre-wound copper coils could then be slid into place, and straight iron plates laid in to create a closed magnetic circuit. Westinghouse obtained a patent for the new low-cost design in 1887.  \\n72  \\n[  \\n]\"),\n",
       " Document(metadata={'Header3': 'Other early transformer designs'}, page_content='Other early transformer designs'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\nIn 1889, Russian-born engineer developed the first transformer at the (\\'General Electricity Company\\') in Germany.  \\nMikhail Dolivo-Dobrovolsky  \\nthree-phase  \\nAllgemeine Elektricitäts-Gesellschaft  \\n84  \\n[  \\n]  \\nIn 1891, invented the , an air-cored, dual-tuned resonant transformer for producing very at high frequency.  \\nNikola Tesla  \\nTesla coil  \\nhigh voltages  \\n85  \\n[  \\n]  \\ntransformers (\" \") were used by early experimenters in the development of the .  \\nAudio frequency  \\nrepeating coils  \\ntelephone  \\n86  \\n[  \\n]'),\n",
       " Document(metadata={'Header2': 'See also'}, page_content='See also'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\n.mw-parser-output .div-col{margin-top:0.3em;column-width:30em}.mw-parser-output .div-col-small{font-size:90%}.mw-parser-output .div-col-rules{column-rule:1px solid #aaa}.mw-parser-output .div-col dl,.mw-parser-output .div-col ol,.mw-parser-output .div-col ul{margin-top:0}.mw-parser-output .div-col li,.mw-parser-output .div-col dd{page-break-inside:avoid;break-inside:avoid-column}  \\nHigh-voltage transformer fire barriers  \\nInductive coupling  \\nLoad profile  \\nMagnetization  \\nParametric transformer  \\nPolyphase system  \\nPower inverter  \\nRectiformer  \\nVoltage converter'),\n",
       " Document(metadata={'Header2': 'Notes'}, page_content='Notes'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\n.mw-parser-output .reflist{margin-bottom:0.5em;list-style-type:decimal}@media screen{.mw-parser-output .reflist{font-size:90%}}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}  \\n^  \\nWith turns of the winding oriented perpendicularly to the magnetic field lines, the flux is the product of the and the core area, the magnetic field varying with time according to the excitation of the primary. The expression , defined as the derivative of magnetic flux with time , provides a measure of rate of magnetic flux in the core and hence of EMF induced in the respective winding. The negative sign in eq. 1 and eq. 2 is consistent with Lenz\\'s law and Faraday\\'s law in that by convention EMF \"induced by an of magnetic flux linkages is to the direction that would be given by the .\"  \\nmagnetic flux density  \\nd  \\nΦ Φ  \\n/  \\nd  \\nt  \\n{\\\\displaystyle \\\\mathrm {d} \\\\Phi /\\\\mathrm {d} t}  \\nΦ Φ  \\n{\\\\displaystyle \\\\Phi }  \\nt  \\n{\\\\displaystyle t}  \\nincrease  \\nopposite  \\nright-hand rule  \\n^  \\nAlthough ideal transformer\\'s winding inductances are each infinitely high, the square root of winding inductances\\' ratio is equal to the turns ratio.  \\n^  \\nThis also implies the following: The net core flux is zero, the input impedance is infinite when secondary is open and zero when secondary is shorted; there is zero phase-shift through an ideal transformer; input and output power and reactive volt-ampere are each conserved; these three statements apply for any frequency above zero and periodic waveforms are conserved.  \\n6  \\n[  \\n]  \\n^  \\nDirection of transformer currents is according to  \\nthe Right-Hand Rule.  \\n^  \\nPercent impedance is the ratio of the voltage drop in the secondary from no load to full load.  \\n15  \\n[  \\n]'),\n",
       " Document(metadata={'Header2': 'References'}, page_content='References'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\nNewPP limit report\\nParsed by mw‐web.codfw.main‐58d4fd4d5d‐pls2x\\nCached time: 20250820024216\\nCache expiry: 2592000\\nReduced expiry: false\\nComplications: [vary‐revision‐sha1, show‐toc]\\nCPU time usage: 1.418 seconds\\nReal time usage: 1.728 seconds\\nPreprocessor visited node count: 8444/1000000\\nRevision size: 85989/2097152 bytes\\nPost‐expand include size: 258348/2097152 bytes\\nTemplate argument size: 8879/2097152 bytes\\nHighest expansion depth: 16/100\\nExpensive parser function count: 7/500\\nUnstrip recursion depth: 1/20\\nUnstrip post‐expand size: 344185/5000000 bytes\\nLua time usage: 0.833/10.000 seconds\\nLua memory usage: 9348626/52428800 bytes\\nNumber of Wikibase entities loaded: 1/500 Transclusion expansion time report (%,ms,calls,template)\\n100.00% 1354.199      1 -total\\n 40.15%  543.741      2 Template:Reflist\\n 16.02%  216.933     36 Template:Cite_book\\n 15.39%  208.360     23 Template:Cite_web\\n  8.67%  117.399      5 Template:Navbox\\n  8.66%  117.293      3 Template:Side_box\\n  6.98%   94.574      1 Template:Electric_transformers\\n  6.82%   92.416     14 Template:Cite_journal\\n  6.57%   88.932     28 Template:Harvnb\\n  6.00%   81.254      1 Template:Short_description Saved in parser cache with key enwiki:pcache:30906:|#|:idhash:canonical and timestamp 20250820024216 and revision id 1304578073. Rendering was triggered because: page-view  \\n^  \\n.mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:\"\\\\\"\"\"\\\\\"\"\"\\'\"\"\\'\"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free.id-lock-free a{background:url(\"//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg\")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited.id-lock-limited a,.mw-parser-output .id-lock-registration.id-lock-registration a{background:url(\"//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg\")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription.id-lock-subscription a{background:url(\"//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg\")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:url(\"//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg\")right 0.1em center/12px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-free a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-limited a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-registration a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-subscription a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .cs1-ws-icon a{background-size:contain;padding:0 1em 0 0}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:var(--color-error,#d33)}.mw-parser-output .cs1-visible-error{color:var(--color-error,#d33)}.mw-parser-output .cs1-maint{display:none;color:#085;margin-left:0.3em}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}@media screen{.mw-parser-output .cs1-format{font-size:95%}html.skin-theme-clientpref-night .mw-parser-output .cs1-maint{color:#18911f}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .cs1-maint{color:#18911f}}  \\n. .  \\n\"Archives Michael Faraday biography – The IET\"  \\ntheiet.org  \\n^  \\nBedell, Frederick (1942). \"History of A-C wave Form, Its Determination and Standardization\". . (12): 864. : . .  \\nTransactions of the American Institute of Electrical Engineers  \\n61  \\ndoi  \\n10.1109/T-AIEE.1942.5058456  \\nS2CID  \\n51658522  \\n^  \\np. 39  \\nSkilling, Hugh Hildreth (1962). . John Wiley & Sons, Inc.  \\nElectromechanics  \\n^  \\na  \\nb  \\n, §18-6 The Ideal Transformer, pp. 598–600  \\nBrenner & Javid 1959  \\n^  \\n, §18-1 Symbols and Polarity of Mutual Inductance, pp.=589–590  \\nBrenner & Javid 1959  \\n^  \\n, p.\\xa0145  \\nCrosby 1958  \\n^  \\nPaul A. Tipler, , Worth Publishers, Inc., 1976 , pp. 937–940  \\nPhysics  \\nISBN  \\n0-87901-041-X  \\n^  \\npp. 2-1, 2-2  \\nFlanagan, William M. (1993). (2nd\\xa0ed.). McGraw-Hill. .  \\nHandbook of Transformer Design & Applications  \\nISBN  \\n978-0-07-021291-6  \\n^  \\n. Saunders College Publishing. 1984. p.\\xa0610. .  \\nElectrical Engineering: An Introduction  \\nISBN  \\n0-03-061758-8  \\n^  \\na  \\nb  \\nc  \\nd  \\ne  \\nf  \\ng  \\nh  \\ni  \\nSay, M. G. (1983). (5th\\xa0ed.). London: Pitman. .  \\nAlternating Current Machines  \\nISBN  \\n978-0-273-01969-5  \\n^  \\nL. Dalessandro, F. d. S. Cavalcante, and J. W. Kolar, \"Self-Capacitance of High-Voltage Transformers,\" IEEE Transactions on Power Electronics, vol. 22, no. 5, pp. 2081–2092, 2007.  \\n^  \\na  \\nb  \\n, pp.\\xa068–74  \\nMcLaren 1984  \\n^  \\na  \\nb  \\nc  \\nd  \\nCalvert, James (2001). . University of Denver. Archived from on May 9, 2007 .  \\n\"Inside Transformers\"  \\nthe original  \\n. Retrieved 2007  \\nMay 19,  \\n^  \\nTerman, Frederick E. (1955). (4th\\xa0ed.). New York: McGraw-Hill. pp. .  \\nElectronic and Radio Engineering  \\n15  \\n^  \\n, p.\\xa04  \\nHeathcote 1998  \\n^  \\nNomenclature for Parallel Operation, pp. 585–586  \\nKnowlton, A.E., ed. (1949). (8th\\xa0ed.). McGraw-Hill. p.\\xa0see esp. Section 6 Transformers, etc, pp. 547–644.  \\nStandard Handbook for Electrical Engineers  \\n^  \\na  \\nb  \\nc  \\n, pp.\\xa047–49  \\nDaniels 1985  \\n^  \\n. .  \\n\"400 Hz Electrical Systems\"  \\nAerospaceweb.org  \\n. Retrieved 2007  \\nMay 21,  \\n^  \\na  \\nb  \\nc  \\nd  \\nDe Keulenaer et al. 2001  \\n^  \\nKubo, T.; Sachs, H.; Nadel, S. (2001). . . p. 39, fig. 1 .  \\nOpportunities for New Appliance and Equipment Efficiency Standards  \\nAmerican Council for an Energy-Efficient Economy  \\n. Retrieved 2009  \\nJune 21,  \\n^  \\na  \\nb  \\nc  \\n, pp.\\xa041–42  \\nHeathcote 1998  \\n^  \\n. FP. Archived from on 10 May 2006 .  \\n\"Understanding Transformer Noise\"  \\n(PDF)  \\nthe original  \\n(PDF)  \\n. Retrieved 2013  \\n30 January  \\n^  \\na  \\nb  \\nNailen, Richard (May 2005). . . Archived from on 2009-04-29.  \\n\"Why We Must Be Concerned With Transformers\"  \\nElectrical Apparatus  \\nthe original  \\n^  \\n, p.\\xa023  \\nPansini 1999  \\n^  \\na  \\nb  \\nc  \\n, pp.\\xa010–11, Fig. 1.8  \\nDel Vecchio et al. 2002  \\n^  \\nHydroelectric Research and Technical Services Group. . U.S. Dept. of the Interior, Bureau of Reclamation. p.\\xa012 .  \\n\"Transformers: Basics, Maintenance, and Diagnostics\"  \\n(PDF)  \\n. Retrieved 2012  \\nMar 27,  \\n^  \\na  \\nb  \\nUS Army Corps of Engineers (1994). . . p.\\xa04-1.  \\n\"EM 1110-2-3006 Engineering and Design – Hydroelectric Power Plants Electrical Design\"  \\nChapter 4 Power Transformers  \\n^  \\na  \\nb  \\n, pp.\\xa029–31  \\nHindmarsh 1977  \\n^  \\n, p.\\xa04  \\nGottlieb 1998  \\n^  \\na  \\nb  \\nAllan, D.J. (Jan 1991). \"Power Transformers – The Second Century\". . (1): 14. : (inactive 12 July 2025).  \\nPower Engineering Journal  \\n5  \\n5–  \\ndoi  \\n10.1049/pe:19910004  \\n:  CS1 maint: DOI inactive as of July 2025 ( )  \\n{{ }}  \\ncite journal  \\nlink  \\n^  \\n, pp.\\xa036–37  \\nKulkarni & Khaparde 2004  \\n^  \\na  \\nb  \\nc  \\nd  \\n, pp.\\xa03-9 to 3-14  \\nMcLyman 2004  \\n^  \\na  \\nb  \\n, §2.1.7 & §2.1.6.2.1 in Section §2.1 Power Transformers by H. Jin Sim and Scott H. Digby in Chapter 2 Equipment Types  \\nHarlow 2004  \\n^  \\nBoteler, D. H.; Pirjola, R. J.; Nevanlinna, H. (1998). \"The Effects of Geomagnetic Disturbances On Electrical Systems at the Earth\\'s Surface\". . (1): 27. : . : .  \\nAdvances in Space Research  \\n22  \\n17–  \\nBibcode  \\n1998AdSpR..22...17B  \\ndoi  \\n10.1016/S0273-1177(97)01096-X  \\n^  \\nHasegawa, Ryusuke (June 2, 2000). \"Present Status of Amorphous Soft Magnetic Alloys\". . 216 (1): 245. : . : .  \\nJournal of Magnetism and Magnetic Materials  \\n215–  \\n240–  \\nBibcode  \\n2000JMMM..215..240H  \\ndoi  \\n10.1016/S0304-8853(00)00126-8  \\n^  \\n, p.\\xa03-1  \\nMcLyman 2004  \\n^  \\n. . Archived from on 2016-09-24 .  \\n\"Toroidal Line Power Transformers. Power Ratings Tripled. | Magnetics Magazine\"  \\nwww.magneticsmagazine.com  \\nthe original  \\n. Retrieved  \\n2016-09-23  \\n^  \\nLee, Reuben. . .  \\n\"Air-Core Transformers\"  \\nElectronic Transformers and Circuits  \\n. Retrieved 2007  \\nMay 22,  \\n^  \\na  \\nb  \\nc  \\nCEGB 1982  \\n^  \\nDixon, Lloyd (2001). . . Texas Instruments.  \\n\"Power Transformer Design\"  \\n(PDF)  \\nMagnetics Design Handbook  \\n^  \\na  \\nb  \\n, §3.4.8 in Section 3.4 Load and Thermal Performance by Robert F. Tillman in Chapter 3 Ancillary Topics  \\nHarlow 2004  \\n^  \\n, p.\\xa032  \\nPansini 1999  \\n^  \\na  \\nb  \\nH. Lee  Willis, , 2004 CRC Press. , pg. 403  \\nPower Distribution Planning Reference Book  \\nISBN  \\n978-0-8247-4875-3  \\n^  \\nHartley, William H. (2003). . 36th Annual Conference of the International Association of Engineering Insurers. p.\\xa07 (fig. 6). Archived from on 20 October 2013 .  \\nAnalysis of Transformer Failures  \\nthe original  \\n. Retrieved 2013  \\n30 January  \\n:  CS1 maint: numeric names: authors list ( )  \\n{{ }}  \\ncite conference  \\nlink  \\n^  \\nHartley, William H. (~2011). . The Locomotive. Archived from on 18 June 2018 .  \\n\"An Analysis of Transformer Failures, Part 1 – 1988 through 1997\"  \\nthe original  \\n. Retrieved 2013  \\n30 January  \\n:  CS1 maint: numeric names: authors list ( )  \\n{{ }}  \\ncite web  \\nlink  \\n^  \\n. 2001 .  \\n\"ASTDR ToxFAQs for Polychlorinated Biphenyls\"  \\n. Retrieved 2007  \\nJune 10,  \\n^  \\na  \\nb  \\n, pp.\\xa02–3  \\nKulkarni & Khaparde 2004  \\n^  \\n. .  \\n\"What silicone wristbands say about chemical exposure in Uruguayan children\"  \\nwww.buffalo.edu  \\n. Retrieved  \\n2022-01-28  \\n^  \\nMehta, S.P.; Aversa, N.; Walker, M.S. (Jul 1997). . . (7): 49. : .  \\n\"Transforming transformers [superconducting windings]\"  \\n(PDF)  \\nIEEE Spectrum  \\n34  \\n43–  \\ndoi  \\n10.1109/6.609815  \\n. Retrieved 2012  \\n14 November  \\n^  \\n, pp.\\xa066–67  \\nPansini 1999  \\n^  \\nLane, Keith (2007) (June 2007). . EC&M .  \\n\"The Basics of Large Dry-Type Transformers\"  \\n. Retrieved 2013  \\n29 January  \\n:  CS1 maint: numeric names: authors list ( )  \\n{{ }}  \\ncite web  \\nlink  \\n^  \\n, pp.\\xa0416–417  \\nRyan 2004  \\n^  \\n. .  \\n\"Guide to Transformer Harmonics and K-factor\"  \\nMaddox  \\n^  \\n, p.\\xa01  \\nHeathcote 1998  \\n^  \\nPoyser, Arthur William (1892). . London and New York: Longmans, Green, & Co. p. , fig. 248.  \\nMagnetism and Electricity: A Manual for Students in Advanced Classes  \\n285  \\n^  \\n.  \\n\"A Brief History of Electromagnetism\"  \\n(PDF)  \\n^  \\n. .  \\n\"Electromagnetism\"  \\nSmithsonian Institution Archives  \\n^  \\nMacPherson, Ph.D., Ryan C. . Archived from on 2015-12-08 .  \\nJoseph Henry: The Rise of an American scientist  \\nthe original  \\n. Retrieved  \\n2015-10-28  \\n^  \\na  \\nb  \\nc  \\nd  \\n, pp.\\xa056–59  \\nGuarnieri 2013  \\n^  \\nChow, Tai L. (2006). . Sudbury, Mass.: Jones and Bartlett Publishers. p.\\xa0171. .  \\nIntroduction to Electromagnetic Theory: A Modern Perspective  \\nISBN  \\n978-0-7637-3827-3  \\n^  \\n(1834). . . : 122. : . .  \\nFaraday, Michael  \\n\"Experimental Researches on Electricity, 7th Series\"  \\nPhilosophical Transactions of the Royal Society  \\n124  \\n77–  \\ndoi  \\n10.1098/rstl.1834.0008  \\nS2CID  \\n116224057  \\n^  \\n. Archived from on 2017-10-11 .  \\n\"Stanley Transformer – 1886 - MagLab\"  \\nthe original  \\n. Retrieved  \\n2021-07-27  \\n^  \\na  \\nb  \\n, pp.\\xa095–96  \\nHughes 1993  \\n^  \\na  \\nb  \\nc  \\nd  \\nUppenborn, F. J. (1889). . London: E. & F. N. Spon. pp. –41.  \\nHistory of the Transformer  \\n35  \\n^  \\nHalacsy, Andrew; Fuchs, George (April 1961). \"Transformer Invented 75 Years Ago\". . (3): 125. : . .  \\nTransactions of the American Institute of Electrical Engineers. Part III: Power Apparatus and Systems  \\n80  \\n121–  \\ndoi  \\n10.1109/AIEEPAS.1961.4500994  \\nS2CID  \\n51632693  \\n^  \\n, pp.\\xa086–95  \\nColtman 1988  \\n^  \\na  \\nb  \\n, pp.\\xa0121–125  \\nHalacsy & Von Fuchs 1961  \\n^  \\nLucas, J.R. . IEE Sri Lanka Centre .  \\n\"Historical Development of the Transformer\"  \\n(PDF)  \\n. Retrieved 2012  \\nMar 1,  \\n^  \\nJeszenszky, Sándor. . . Archived from the original on June 27, 2022 .  \\n\"Electrostatics and Electrodynamics at Pest University in the Mid-19th Century\"  \\n(PDF)  \\nUniversity of Pavia  \\n. Retrieved 2012  \\nMar 3,  \\n:  CS1 maint: bot: original URL status unknown ( )  \\n{{ }}  \\ncite web  \\nlink  \\n^  \\n. Institute for Developing Alternative Energy in Latin America. Archived from on 2012-03-22 .  \\n\"Hungarian Inventors and Their Inventions\"  \\nthe original  \\n. Retrieved 2012  \\nMar 3,  \\n^  \\n. Budapest University of Technology and Economics, National Technical Information Centre and Library .  \\n\"Bláthy, Ottó Titusz\"  \\n. Retrieved 2012  \\nFeb 29,  \\n^  \\na  \\nb  \\nSmil, Vaclav (2005). . Oxford: Oxford University Press. p. . .  \\nCreating the Twentieth Century: Technical Innovations of 1867–1914 and Their Lasting Impact  \\n71  \\nISBN  \\n978-0-19-803774-3  \\nZBD transformer.  \\n^  \\nElectrical Society of Cornell University (1896). . Andrus & Church. p.\\xa039.  \\nProceedings of the Electrical Society of  \\nCornell University  \\n^  \\nNagy, Árpád Zoltán (Oct 11, 1996). . Budapest. Archived from on November 25, 2012 .  \\n\"Lecture to Mark the 100th Anniversary of the Discovery of the Electron in 1897 (preliminary text)\"  \\nthe original  \\n. Retrieved 2009  \\nJuly 9,  \\n^  \\n(2nd\\xa0ed.). Oxford University Press. 1989.  \\nOxford English Dictionary  \\n^  \\nHospitalier, Édouard (1882). . Translated by Julius Maier. New York: D. Appleton & Co. p. .  \\nThe Modern Applications of Electricity  \\n103  \\n^  \\n. IEC Techline. Archived from on 2010-12-06 .  \\n\"Ottó Bláthy, Miksa Déri, Károly Zipernowsky\"  \\nthe original  \\n. Retrieved 2010  \\nApr 16,  \\n^  \\nBrusso, Barry; Allerhand, Adam (January 2021). . . . IEEE: 12. : . .  \\n\"A Contrarian History of Early Electric Power Distribution [History]\"  \\nIEEE Industry Applications Magazine  \\n27  \\ndoi  \\n10.1109/MIAS.2020.3028630  \\nS2CID  \\n230605234  \\n^  \\n. Tinicum Township Historical Society. 1993. from the original on April 23, 2015.  \\nHistory of Tinicum Township (PA) 1643–1993  \\n(PDF)  \\nArchived  \\n(PDF)  \\n^  \\nWilliam R. Huber (2022). . . p.\\xa084. .  \\nGeorge Westinghouse Powering the World  \\nMcFarland & Company  \\nISBN  \\n9781476686929  \\n^  \\nSkrabec, Quentin R. (2007). . Algora Publishing. p.\\xa0102. .  \\nGeorge Westinghouse: Gentle Genius  \\nISBN  \\n978-0-87586-508-9  \\n^  \\na  \\nb  \\nColtman 2002  \\n^  \\n. . Archived from on December 6, 2010 .  \\nInternational Electrotechnical Commission  \\nOtto Blathy, Miksa Déri, Károly Zipernowsky  \\nthe original  \\n. Retrieved 2007  \\nMay 17,  \\n:  \\n{{ }}  \\ncite book  \\nignored ( )  \\n|work=  \\nhelp  \\n^  \\nNeidhöfer, Gerhard (2008). (in German). In collaboration with VDE \"History of Electrical Engineering\" Committee (2nd\\xa0ed.). Berlin: VDE-Verl. .  \\nMichael von Dolivo-Dobrowolsky and Three-Phase: The Beginnings of Modern e Technology and Power Supply  \\nISBN  \\n978-3-8007-3115-2  \\n^  \\nUth, Robert (Dec 12, 2000). . . PBS.org .  \\n\"Tesla Coil\"  \\nTesla: Master of Lightning  \\n. Retrieved 2008  \\nMay 20,  \\n^  \\n. .  \\n\"telephone | History, Definition, Invention, Uses, & Facts | Britannica\"  \\nwww.britannica.com  \\n. Retrieved  \\n2022-07-17'),\n",
       " Document(metadata={'Header2': 'Bibliography'}, page_content='Bibliography'),\n",
       " Document(metadata={'Header2': 'Bibliography'}, page_content='[  \\nedit  \\n]  \\nBeeman, Donald, ed. (1955). . McGraw-Hill.  \\nIndustrial Power Systems Handbook  \\nCalvert, James (2001). . University of Denver. Archived from on May 9, 2007 .  \\n\"Inside Transformers\"  \\nthe original  \\n. Retrieved 2007  \\nMay 19,  \\nColtman, J. W. (Jan 1988). \"The Transformer\". . (1): 95. : . : . .  \\nScientific American  \\n258  \\n86–  \\nBibcode  \\n1988SciAm.258a..86C  \\ndoi  \\n10.1038/scientificamerican0188-86  \\nOSTI  \\n6851152  \\nColtman, J. W. (Jan–Feb 2002). \"History: The Transformer\". . (1): 15. : . .  \\nIEEE Industry Applications Magazine  \\n8  \\n8–  \\ndoi  \\n10.1109/2943.974352  \\nS2CID  \\n18160717  \\nBrenner, Egon; Javid, Mansour (1959). . . McGraw-Hill. pp. 622.  \\n\"Chapter 18–Circuits with Magnetic Coupling\"  \\nAnalysis of Electric Circuits  \\n586–  \\nCEGB, (Central Electricity Generating Board) (1982). . Pergamon. .  \\nModern Power Station Practice  \\nISBN  \\n978-0-08-016436-6  \\nCrosby, D. (1958). \"The Ideal Transformer\". . (2): 145. : .  \\nIRE Transactions on Circuit Theory  \\n5  \\ndoi  \\n10.1109/TCT.1958.1086447  \\nDaniels, A. R. (1985). . Macmillan. .  \\nIntroduction to Electrical Machines  \\nISBN  \\n978-0-333-19627-4  \\nDe Keulenaer, Hans; Chapman, David; Fassbinder, Stefan; McDermott, Mike (2001). . 16th International Conference and Exhibition on Electricity Distribution (CIRED 2001). Institution of Engineering and Technology. : . Archived from on 4 March 2016 .  \\nThe Scope for Energy Saving in the EU through the Use of Energy-Efficient Electricity Distribution Transformers  \\n(PDF)  \\ndoi  \\n10.1049/cp:20010853  \\nthe original  \\n(PDF)  \\n. Retrieved 2014  \\n10 July  \\nDel Vecchio, Robert M.; Poulin, Bertrand; Feghali, Pierre T.M.; Shah, Dilipkumar; Ahuja, Rajendra (2002). . Boca Raton: CRC Press. .  \\nTransformer Design Principles: With Applications to Core-Form Power Transformers  \\nISBN  \\n978-90-5699-703-8  \\nFink, Donald G.; Beatty, H. Wayne, eds. (1978). (11th\\xa0ed.). McGraw Hill. .  \\nStandard Handbook for Electrical Engineers  \\nISBN  \\n978-0-07-020974-9  \\nGottlieb, Irving (1998). . Elsevier. .  \\nPractical Transformer Handbook: for Electronics, Radio and Communications Engineers  \\nISBN  \\n978-0-7506-3992-7  \\nGuarnieri, M. (2013). \"Who Invented the Transformer?\". . (4): 59. : . .  \\nIEEE Industrial Electronics Magazine  \\n7  \\n56–  \\ndoi  \\n10.1109/MIE.2013.2283834  \\nS2CID  \\n27936000  \\nHalacsy, A. A.; Von Fuchs, G. H. (April 1961). \"Transformer Invented 75 Years Ago\". . (3): 125. : . .  \\nTransactions of the American Institute of Electrical Engineers. Part III: Power Apparatus and Systems  \\n80  \\n121–  \\ndoi  \\n10.1109/AIEEPAS.1961.4500994  \\nS2CID  \\n51632693  \\nHameyer, Kay (2004). . RWTH Aachen University Institute of Electrical Machines. Archived from on 2013-02-10.  \\nElectrical Machines I: Basics, Design, Function, Operation  \\n(PDF)  \\nthe original  \\n(PDF)  \\nHammond, John Winthrop (1941). . J. B. Lippincott Company. pp.\\xa0see esp. 106–107, 178, 238.  \\nMen and Volts: The Story of General Electric  \\nHarlow, James (2004). . CRC Press. .  \\nElectric Power Transformer Engineering  \\n(PDF)  \\nISBN  \\n0-8493-1704-5  \\n[ ]  \\npermanent dead link  \\nHughes, Thomas P. (1993). . Baltimore: The Johns Hopkins University Press. p.\\xa096. .  \\nNetworks of Power: Electrification in Western Society, 1880-1930  \\nISBN  \\n978-0-8018-2873-7  \\n. Retrieved 2009  \\nSep 9,  \\nHeathcote, Martin (1998). (12th\\xa0ed.). Newnes. .  \\nJ & P Transformer Book  \\nISBN  \\n978-0-7506-1158-9  \\nHindmarsh, John (1977). (4th\\xa0ed.). Exeter: Pergamon. .  \\nElectrical Machines and Their Applications  \\nISBN  \\n978-0-08-030573-8  \\nKothari, D.P.; Nagrath, I.J. (2010). (4th\\xa0ed.). Tata McGraw-Hill. .  \\nElectric Machines  \\nISBN  \\n978-0-07-069967-0  \\nKulkarni, S. V.; Khaparde, S. A. (2004). . CRC Press. .  \\nTransformer Engineering: Design and Practice  \\nISBN  \\n978-0-8247-5653-6  \\nMcLaren, Peter (1984). . Ellis Horwood. .  \\nElementary Electric Power and Machines  \\nISBN  \\n978-0-470-20057-5  \\nMcLyman, Colonel William (2004). . . CRC. .  \\n\"Chapter 3\"  \\nTransformer and Inductor Design Handbook  \\nISBN  \\n0-8247-5393-3  \\nPansini, Anthony (1999). . CRC Press. .  \\nElectrical Transformers and Power Equipment  \\nISBN  \\n978-0-88173-311-2  \\nParker, M. R; Ula, S.; Webb, W. E. (2005). . In Whitaker, Jerry C. (ed.). (2nd\\xa0ed.). Taylor & Francis. pp.\\xa0172, 1017. .  \\n\"§2.5.5 \\'Transformers\\' & §10.1.3 \\'The Ideal Transformer\\' \"  \\nThe Electronics Handbook  \\nISBN  \\n0-8493-1889-0  \\nRyan, H. M. (2004). . CRC Press. .  \\nHigh Voltage Engineering and Testing  \\nISBN  \\n978-0-85296-775-1'),\n",
       " Document(metadata={'Header2': 'External links'}, page_content='External links'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\n@media print{body.ns-0 .mw-parser-output .sistersitebox{display:none!important}}@media screen{html.skin-theme-clientpref-night .mw-parser-output .sistersitebox img[src*=\"Wiktionary-logo-en-v2.svg\"]{background-color:white}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .sistersitebox img[src*=\"Wiktionary-logo-en-v2.svg\"]{background-color:white}}  \\nWikimedia Commons has media related to .  \\nTransformers  \\nThe Wikibook has a page on the topic of:  \\nSchool Science  \\nHow to make a transformer  \\n:  \\nGeneral links  \\n(Video) Power transformer inrush current (animation)  \\n.mw-parser-output .hlist dl,.mw-parser-output .hlist ol,.mw-parser-output .hlist ul{margin:0;padding:0}.mw-parser-output .hlist dd,.mw-parser-output .hlist dt,.mw-parser-output .hlist li{margin:0;display:inline}.mw-parser-output .hlist.inline,.mw-parser-output .hlist.inline dl,.mw-parser-output .hlist.inline ol,.mw-parser-output .hlist.inline ul,.mw-parser-output .hlist dl dl,.mw-parser-output .hlist dl ol,.mw-parser-output .hlist dl ul,.mw-parser-output .hlist ol dl,.mw-parser-output .hlist ol ol,.mw-parser-output .hlist ol ul,.mw-parser-output .hlist ul dl,.mw-parser-output .hlist ul ol,.mw-parser-output .hlist ul ul{display:inline}.mw-parser-output .hlist .mw-empty-li{display:none}.mw-parser-output .hlist dt::after{content:\": \"}.mw-parser-output .hlist dd::after,.mw-parser-output .hlist li::after{content:\" · \";font-weight:bold}.mw-parser-output .hlist dd:last-child::after,.mw-parser-output .hlist dt:last-child::after,.mw-parser-output .hlist li:last-child::after{content:none}.mw-parser-output .hlist dd dd:first-child::before,.mw-parser-output .hlist dd dt:first-child::before,.mw-parser-output .hlist dd li:first-child::before,.mw-parser-output .hlist dt dd:first-child::before,.mw-parser-output .hlist dt dt:first-child::before,.mw-parser-output .hlist dt li:first-child::before,.mw-parser-output .hlist li dd:first-child::before,.mw-parser-output .hlist li dt:first-child::before,.mw-parser-output .hlist li li:first-child::before{content:\" (\";font-weight:normal}.mw-parser-output .hlist dd dd:last-child::after,.mw-parser-output .hlist dd dt:last-child::after,.mw-parser-output .hlist dd li:last-child::after,.mw-parser-output .hlist dt dd:last-child::after,.mw-parser-output .hlist dt dt:last-child::after,.mw-parser-output .hlist dt li:last-child::after,.mw-parser-output .hlist li dd:last-child::after,.mw-parser-output .hlist li dt:last-child::after,.mw-parser-output .hlist li li:last-child::after{content:\")\";font-weight:normal}.mw-parser-output .hlist ol{counter-reset:listitem}.mw-parser-output .hlist ol>li{counter-increment:listitem}.mw-parser-output .hlist ol>li::before{content:\" \"counter(listitem)\"\\\\a0 \"}.mw-parser-output .hlist dd ol>li:first-child::before,.mw-parser-output .hlist dt ol>li:first-child::before,.mw-parser-output .hlist li ol>li:first-child::before{content:\" (\"counter(listitem)\"\\\\a0 \"}  \\n.mw-parser-output .navbox{box-sizing:border-box;border:1px solid #a2a9b1;width:100%;clear:both;font-size:88%;text-align:center;padding:1px;margin:1em auto 0}.mw-parser-output .navbox .navbox{margin-top:0}.mw-parser-output .navbox+.navbox,.mw-parser-output .navbox+.navbox-styles+.navbox{margin-top:-1px}.mw-parser-output .navbox-inner,.mw-parser-output .navbox-subgroup{width:100%}.mw-parser-output .navbox-group,.mw-parser-output .navbox-title,.mw-parser-output .navbox-abovebelow{padding:0.25em 1em;line-height:1.5em;text-align:center}.mw-parser-output .navbox-group{white-space:nowrap;text-align:right}.mw-parser-output .navbox,.mw-parser-output .navbox-subgroup{background-color:#fdfdfd}.mw-parser-output .navbox-list{line-height:1.5em;border-color:#fdfdfd}.mw-parser-output .navbox-list-with-group{text-align:left;border-left-width:2px;border-left-style:solid}.mw-parser-output tr+tr>.navbox-abovebelow,.mw-parser-output tr+tr>.navbox-group,.mw-parser-output tr+tr>.navbox-image,.mw-parser-output tr+tr>.navbox-list{border-top:2px solid #fdfdfd}.mw-parser-output .navbox-title{background-color:#ccf}.mw-parser-output .navbox-abovebelow,.mw-parser-output .navbox-group,.mw-parser-output .navbox-subgroup .navbox-title{background-color:#ddf}.mw-parser-output .navbox-subgroup .navbox-group,.mw-parser-output .navbox-subgroup .navbox-abovebelow{background-color:#e6e6ff}.mw-parser-output .navbox-even{background-color:#f7f7f7}.mw-parser-output .navbox-odd{background-color:transparent}.mw-parser-output .navbox .hlist td dl,.mw-parser-output .navbox .hlist td ol,.mw-parser-output .navbox .hlist td ul,.mw-parser-output .navbox td.hlist dl,.mw-parser-output .navbox td.hlist ol,.mw-parser-output .navbox td.hlist ul{padding:0.125em 0}.mw-parser-output .navbox .navbar{display:block;font-size:100%}.mw-parser-output .navbox-title .navbar{float:left;text-align:left;margin-right:0.5em}body.skin--responsive .mw-parser-output .navbox-image img{max-width:none!important}@media print{body.ns-0 .mw-parser-output .navbox{display:none!important}}  \\n.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:\"[ \"}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:\" ]\"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}html.skin-theme-clientpref-night .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}}@media print{.mw-parser-output .navbar{display:none!important}}  \\nv  \\nt  \\ne  \\ntopics  \\nTransformer  \\nTopics  \\nBalun  \\nBuchholz relay  \\nBushing  \\nCenter tap  \\nCircle diagram  \\nCondition monitoring of transformers  \\nElectrical insulation paper  \\nGrowler  \\nHigh-leg delta  \\nInduction regulator  \\nLeakage inductance  \\nMagnet wire  \\nMetadyne  \\nOpen-circuit test  \\nPolarity  \\nPressure relief valve  \\nQuadrature booster  \\nResolver  \\nResonant inductive coupling  \\nSeverity factor  \\nShort-circuit test  \\nStacking factor  \\nSynchro  \\nTap changer  \\nToroidal inductors and transformers  \\nTransformer oil  \\nDissolved gas analysis  \\nTransformer oil testing  \\nTransformer utilization factor  \\nVector group  \\nTypes  \\nAutotransformer  \\nBuck–boost transformer  \\nDistribution transformer  \\nPad-mounted transformer  \\nDelta-wye transformer  \\nEnergy efficient transformer  \\nAmorphous metal transformer  \\nFlyback transformer  \\nGrounding transformer  \\nInstrument transformer  \\nCurrent transformer  \\nVoltage transformer  \\nIsolation transformer  \\nAustin transformer  \\nLinear variable differential transformer  \\nParametric transformer  \\nPlanar transformer  \\nRotary transformer  \\nRotary variable differential transformer  \\nScott-T transformer  \\nSolid-state transformer  \\nTrigger transformer  \\nVariable-frequency transformer  \\nZigzag transformer  \\nCoils  \\nHybrid coil  \\nInduction coil  \\nOudin coil  \\nPolyphase coil  \\nRepeating coil  \\nTesla coil  \\nTrembler coil  \\nManufacturers  \\nABB  \\nGeneral Electric  \\nMitsubishi Electric  \\nProlecGE  \\nSchneider Electric  \\nSiemens  \\nTBEA  \\nWEG  \\nv  \\nt  \\ne  \\nElectricity delivery  \\nConcepts  \\nAutomatic generation control  \\nBackfeeding  \\nBase load  \\nDemand factor  \\nDroop speed control  \\nElectric power  \\nElectric power quality  \\nElectrical fault  \\nEnergy demand management  \\nEnergy return on investment  \\nGrid code  \\nGrid energy storage  \\nGrid strength  \\nHome energy storage  \\nLoad-following  \\nMerit order  \\nNameplate capacity  \\nPeak demand  \\nPower factor  \\nPower-flow study  \\nPower system reliability  \\nRepowering  \\nUtility frequency  \\nVariability  \\nVehicle-to-grid  \\nSources  \\nNon-renewable  \\nFossil fuel power station  \\nCoal  \\nNatural gas  \\nOil shale  \\nPetroleum  \\nNuclear  \\nRenewable  \\nBiofuel  \\nBiogas  \\nBiomass  \\nGeothermal  \\nHydro  \\nMarine  \\nCurrent  \\nOsmotic  \\nThermal  \\nTidal  \\nWave  \\nSolar  \\nSustainable biofuel  \\nWind  \\nGeneration  \\nAC power  \\nCogeneration  \\nCombined cycle  \\nCooling tower  \\nDispatchable  \\nEnergy storage  \\nBattery  \\nInduction generator  \\nInertial response  \\nInverter-based resource  \\nMicro CHP  \\nMicrogeneration  \\nRankine cycle  \\nThree-phase electric power  \\nVirtual power plant  \\nand  \\nTransmission  \\ndistribution  \\nAncillary services  \\nBalancing authority  \\nContingency (electrical grid)  \\nDemand response  \\nDistributed generation  \\nDynamic demand  \\nElectric power distribution  \\nElectric power system  \\nElectric power transmission  \\nElectrical busbar system  \\nElectrical grid  \\nElectricity retailing  \\nGrid balancing  \\nHigh-voltage direct current  \\nHigh-voltage shore connection  \\nInterconnector  \\nLoad management  \\nMains electricity by country  \\nOverhead power line  \\nConductor gallop  \\nPower station  \\nPumped hydro  \\nSingle-wire earth return  \\nSmart grid  \\nSubstation  \\nSuper grid  \\nTransformer  \\n(TSO)  \\nTransmission system operator  \\nTransmission tower  \\nUtility pole  \\nVoltage control and reactive power management  \\nFailure modes  \\nBlack start  \\nBrownout  \\nCascading failure  \\nIslanding  \\nPower outage  \\nList  \\nRolling blackout  \\nProtective devices  \\nArc-fault circuit interrupter  \\nCircuit breaker  \\nEarth-leakage  \\nSulfur hexafluoride  \\nGenerator interlock kit  \\nNumerical relay  \\nPower system protection  \\nProtective relay  \\n(GFI)  \\nResidual-current device  \\nEconomics and policies  \\nAvailability factor  \\nCapacity factor  \\nCarbon offsets and credits  \\nCost of electricity by source  \\nEnergy subsidies  \\nEnvironmental tax  \\nFeed-in tariff  \\nFossil fuel phase-out  \\nLoad factor  \\nNet metering  \\nPigouvian tax  \\nRenewable Energy Certificates  \\nRenewable energy commercialization  \\nRenewable Energy Payments  \\nSpark/Dark/Quark/Bark spread  \\nStatistics and production  \\nElectric energy consumption  \\nList of electricity sectors  \\nCategory  \\nv  \\nt  \\ne  \\nElectronic components  \\nSemiconductor devices  \\nMOS transistors  \\nTransistor  \\nNMOS  \\nPMOS  \\nBiCMOS  \\nBioFET  \\n(ChemFET)  \\nChemical field-effect transistor  \\n(CMOS)  \\nComplementary MOS  \\nDepletion-load NMOS  \\n(FinFET)  \\nFin field-effect transistor  \\n(FGMOS)  \\nFloating-gate MOSFET  \\n(IGBT)  \\nInsulated-gate bipolar transistor  \\nISFET  \\nLDMOS  \\n(MOSFET)  \\nMOS field-effect transistor  \\n(MuGFET)  \\nMulti-gate field-effect transistor  \\nPower MOSFET  \\n(TFT)  \\nThin-film transistor  \\nVMOS  \\nUMOS  \\nOther transistors  \\n(BJT)  \\nBipolar junction transistor  \\nDarlington transistor  \\nDiffused junction transistor  \\n(FET)  \\nField-effect transistor  \\nJunction Gate FET (JFET)  \\nOrganic FET (OFET)  \\n(LET)  \\nLight-emitting transistor  \\nOrganic LET (OLET)  \\nPentode transistor  \\nPoint-contact transistor  \\n(PUT)  \\nProgrammable unijunction transistor  \\n(SIT)  \\nStatic induction transistor  \\nTetrode transistor  \\n(UJT)  \\nUnijunction transistor  \\nDiodes  \\nAvalanche diode  \\n(CLD, CRD)  \\nConstant-current diode  \\nGunn diode  \\n(LD)  \\nLaser diode  \\n(LED)  \\nLight-emitting diode  \\n(OLED)  \\nOrganic light-emitting diode  \\nPhotodiode  \\nPIN diode  \\nSchottky diode  \\nStep recovery diode  \\nZener diode  \\nOther devices  \\nPrinted electronics  \\nPrinted circuit board  \\nDIAC  \\nHeterostructure barrier varactor  \\n(IC)  \\nIntegrated circuit  \\nHybrid integrated circuit  \\n(LEC)  \\nLight emitting capacitor  \\nMemistor  \\nMemristor  \\nMemtransistor  \\nMemory cell  \\n(MOV)  \\nMetal-oxide varistor  \\nMixed-signal integrated circuit  \\n(MOS IC)  \\nMOS integrated circuit  \\nOrganic semiconductor  \\nPhotodetector  \\nQuantum circuit  \\nRF CMOS  \\n(SCR)  \\nSilicon controlled rectifier  \\nSolaristor  \\n(SITh)  \\nStatic induction thyristor  \\n(3D IC)  \\nThree-dimensional integrated circuit  \\nThyristor  \\nTrancitor  \\nTRIAC  \\nVaricap  \\nVoltage regulators  \\nLinear regulator  \\nLow-dropout regulator  \\nSwitching regulator  \\nBuck  \\nBoost  \\nBuck–boost  \\nSplit-pi  \\nĆuk  \\nSEPIC  \\nCharge pump  \\nSwitched capacitor  \\nVacuum tubes  \\nAcorn tube  \\nAudion  \\nBeam tetrode  \\nBarretter  \\nCompactron  \\nDiode  \\nFleming valve  \\nNeutron tube  \\nNonode  \\nNuvistor  \\n(Hexode, Heptode, Octode)  \\nPentagrid  \\nPentode  \\nPhotomultiplier  \\nPhototube  \\nTetrode  \\nTriode  \\n( )  \\nVacuum tubes  \\nRF  \\n(BWO)  \\nBackward-wave oscillator  \\nCavity magnetron  \\n(CFA)  \\nCrossed-field amplifier  \\nGyrotron  \\n(IOT)  \\nInductive output tube  \\nKlystron  \\nMaser  \\nSutton tube  \\n(TWT)  \\nTraveling-wave tube  \\nX-ray tube  \\nCathode-ray tubes  \\nBeam deflection tube  \\nCharactron  \\nIconoscope  \\nMagic eye tube  \\nMonoscope  \\nSelectron tube  \\nStorage tube  \\nTrochotron  \\nVideo camera tube  \\nWilliams tube  \\nGas-filled tubes  \\nCold cathode  \\nCrossatron  \\nDekatron  \\nIgnitron  \\nKrytron  \\nMercury-arc valve  \\nNeon lamp  \\nNixie tube  \\nThyratron  \\nTrigatron  \\nVoltage-regulator tube  \\nAdjustable  \\nPotentiometer  \\ndigital  \\nVariable capacitor  \\nVaricap  \\nPassive  \\nConnector  \\naudio and video  \\nelectrical power  \\nRF  \\nElectrolytic detector  \\nFerrite  \\nAntifuse  \\nFuse  \\nresettable  \\neFUSE  \\nResistor  \\nSwitch  \\nThermistor  \\nTransformer  \\nVaristor  \\nWire  \\nWollaston wire  \\nReactive  \\nCapacitor  \\ntypes  \\nCeramic resonator  \\nCrystal oscillator  \\nInductor  \\nParametron  \\nRelay  \\nreed relay  \\nmercury relay  \\n.mw-parser-output .tooltip-dotted{border-bottom:1px dotted;cursor:help}  \\nAuthority control databases  \\nNational  \\nGermany  \\nUnited States  \\nFrance  \\nBnF data  \\nJapan  \\nCzech Republic  \\nIsrael  \\nOther  \\nYale LUX  \\nRetrieved from \" \"  \\nhttps://en.wikipedia.org/w/index.php?title=Transformer&oldid=1304578073  \\n:  \\nCategories  \\nElectric power conversion  \\nElectric transformers  \\n19th-century inventions  \\nHungarian inventions  \\nBritish inventions  \\nElectrical engineering  \\nHidden categories:  \\nCS1 maint: DOI inactive as of July 2025  \\nCS1: long volume value  \\nCS1 maint: numeric names: authors list  \\nCS1 maint: bot: original URL status unknown  \\nCS1 errors: periodical ignored  \\nCS1 German-language sources (de)  \\nArticles with short description  \\nShort description is different from Wikidata  \\nAll articles with dead external links  \\nArticles with dead external links from December 2021  \\nArticles with permanently dead external links  \\nCommons category link is on Wikidata  \\nThis page was last edited on 6 August 2025, at 22:17 .  \\n(UTC)  \\nText is available under the ;\\nadditional terms may apply. By using this site, you agree to the and . Wikipedia® is a registered trademark of the , a non-profit organization.  \\nCreative Commons Attribution-ShareAlike 4.0 License  \\nTerms of Use  \\nPrivacy Policy  \\nWikimedia Foundation, Inc.  \\nPrivacy policy  \\nAbout Wikipedia  \\nDisclaimers  \\nContact Wikipedia  \\nCode of Conduct  \\nDevelopers  \\nStatistics  \\nCookie statement  \\nMobile view  \\nSearch  \\nSearch  \\nToggle the table of contents  \\nTransformer  \\n113 languages  \\nAdd topic  \\n(RLQ=window.RLQ||[]).push(function(){mw.config.set({\"wgHostname\":\"mw-web.codfw.main-58d4fd4d5d-mpxlz\",\"wgBackendResponseTime\":157,\"wgPageParseReport\":{\"limitreport\":{\"cputime\":\"1.418\",\"walltime\":\"1.728\",\"ppvisitednodes\":{\"value\":8444,\"limit\":1000000},\"revisionsize\":{\"value\":85989,\"limit\":2097152},\"postexpandincludesize\":{\"value\":258348,\"limit\":2097152},\"templateargumentsize\":{\"value\":8879,\"limit\":2097152},\"expansiondepth\":{\"value\":16,\"limit\":100},\"expensivefunctioncount\":{\"value\":7,\"limit\":500},\"unstrip-depth\":{\"value\":1,\"limit\":20},\"unstrip-size\":{\"value\":344185,\"limit\":5000000},\"entityaccesscount\":{\"value\":1,\"limit\":500},\"timingprofile\":[\"100.00% 1354.199      1 -total\",\" 40.15%  543.741      2 Template:Reflist\",\" 16.02%  216.933     36 Template:Cite_book\",\" 15.39%  208.360     23 Template:Cite_web\",\"  8.67%  117.399      5 Template:Navbox\",\"  8.66%  117.293      3 Template:Side_box\",\"  6.98%   94.574      1 Template:Electric_transformers\",\"  6.82%   92.416     14 Template:Cite_journal\",\"  6.57%   88.932     28 Template:Harvnb\",\"  6.00%   81.254      1 Template:Short_description\"]},\"scribunto\":{\"limitreport-timeusage\":{\"value\":\"0.833\",\"limit\":\"10.000\"},\"limitreport-memusage\":{\"value\":9348626,\"limit\":52428800},\"limitreport-logs\":\"anchor_id_list = table#1 {\\\\n    [\\\\\"CITEREFAllan1991\\\\\"] = 1,\\\\n    [\\\\\"CITEREFBedell1942\\\\\"] = 1,\\\\n    [\\\\\"CITEREFBeeman1955\\\\\"] = 1,\\\\n    [\\\\\"CITEREFBotelerPirjolaNevanlinna1998\\\\\"] = 1,\\\\n    [\\\\\"CITEREFBrennerJavid1959\\\\\"] = 1,\\\\n    [\\\\\"CITEREFBrussoAllerhand2021\\\\\"] = 1,\\\\n    [\\\\\"CITEREFCEGB1982\\\\\"] = 1,\\\\n    [\\\\\"CITEREFCalvert2001\\\\\"] = 2,\\\\n    [\\\\\"CITEREFChow2006\\\\\"] = 1,\\\\n    [\\\\\"CITEREFColtman1988\\\\\"] = 1,\\\\n    [\\\\\"CITEREFColtman2002\\\\\"] = 1,\\\\n    [\\\\\"CITEREFCrosby1958\\\\\"] = 1,\\\\n    [\\\\\"CITEREFDaniels1985\\\\\"] = 1,\\\\n    [\\\\\"CITEREFDe_KeulenaerChapmanFassbinderMcDermott2001\\\\\"] = 1,\\\\n    [\\\\\"CITEREFDel_VecchioPoulinFeghaliShah2002\\\\\"] = 1,\\\\n    [\\\\\"CITEREFDixon2001\\\\\"] = 1,\\\\n    [\\\\\"CITEREFElectrical_Society_of_Cornell_University1896\\\\\"] = 1,\\\\n    [\\\\\"CITEREFFaraday1834\\\\\"] = 1,\\\\n    [\\\\\"CITEREFFinkBeatty1978\\\\\"] = 1,\\\\n    [\\\\\"CITEREFFlanagan1993\\\\\"] = 1,\\\\n    [\\\\\"CITEREFGottlieb1998\\\\\"] = 1,\\\\n    [\\\\\"CITEREFGuarnieri2013\\\\\"] = 1,\\\\n    [\\\\\"CITEREFHalacsyFuchs1961\\\\\"] = 1,\\\\n    [\\\\\"CITEREFHalacsyVon_Fuchs1961\\\\\"] = 1,\\\\n    [\\\\\"CITEREFHameyer2004\\\\\"] = 1,\\\\n    [\\\\\"CITEREFHammond1941\\\\\"] = 1,\\\\n    [\\\\\"CITEREFHarlow2004\\\\\"] = 1,\\\\n    [\\\\\"CITEREFHartley\\\\\"] = 2,\\\\n    [\\\\\"CITEREFHasegawa2000\\\\\"] = 1,\\\\n    [\\\\\"CITEREFHeathcote1998\\\\\"] = 1,\\\\n    [\\\\\"CITEREFHindmarsh1977\\\\\"] = 1,\\\\n    [\\\\\"CITEREFHospitalier1882\\\\\"] = 1,\\\\n    [\\\\\"CITEREFHughes1993\\\\\"] = 1,\\\\n    [\\\\\"CITEREFHydroelectric_Research_and_Technical_Services_Group\\\\\"] = 1,\\\\n    [\\\\\"CITEREFInternational_Electrotechnical_Commission\\\\\"] = 1,\\\\n    [\\\\\"CITEREFJeszenszky\\\\\"] = 1,\\\\n    [\\\\\"CITEREFKnowlton1949\\\\\"] = 1,\\\\n    [\\\\\"CITEREFKothariNagrath2010\\\\\"] = 1,\\\\n    [\\\\\"CITEREFKuboSachsNadel2001\\\\\"] = 1,\\\\n    [\\\\\"CITEREFKulkarniKhaparde2004\\\\\"] = 1,\\\\n    [\\\\\"CITEREFLane2007\\\\\"] = 1,\\\\n    [\\\\\"CITEREFLee\\\\\"] = 1,\\\\n    [\\\\\"CITEREFLucas\\\\\"] = 1,\\\\n    [\\\\\"CITEREFMacPherson,_Ph.D.\\\\\"] = 1,\\\\n    [\\\\\"CITEREFMcLaren1984\\\\\"] = 1,\\\\n    [\\\\\"CITEREFMcLyman2004\\\\\"] = 1,\\\\n    [\\\\\"CITEREFMehtaAversa,_N.Walker,_M.S.1997\\\\\"] = 1,\\\\n    [\\\\\"CITEREFNagy1996\\\\\"] = 1,\\\\n    [\\\\\"CITEREFNailen2005\\\\\"] = 1,\\\\n    [\\\\\"CITEREFNeidhöfer2008\\\\\"] = 1,\\\\n    [\\\\\"CITEREFPansini1999\\\\\"] = 1,\\\\n    [\\\\\"CITEREFParkerUlaWebb2005\\\\\"] = 1,\\\\n    [\\\\\"CITEREFPoyser1892\\\\\"] = 1,\\\\n    [\\\\\"CITEREFRyan2004\\\\\"] = 1,\\\\n    [\\\\\"CITEREFSay1983\\\\\"] = 1,\\\\n    [\\\\\"CITEREFSkilling1962\\\\\"] = 1,\\\\n    [\\\\\"CITEREFSkrabec2007\\\\\"] = 1,\\\\n    [\\\\\"CITEREFSmil2005\\\\\"] = 1,\\\\n    [\\\\\"CITEREFTerman1955\\\\\"] = 1,\\\\n    [\\\\\"CITEREFUS_Army_Corps_of_Engineers1994\\\\\"] = 1,\\\\n    [\\\\\"CITEREFUppenborn1889\\\\\"] = 1,\\\\n    [\\\\\"CITEREFUth2000\\\\\"] = 1,\\\\n    [\\\\\"CITEREFWilliam_R._Huber2022\\\\\"] = 1,\\\\n    [\\\\\"Ideal_transformer_equations\\\\\"] = 1,\\\\n    [\\\\\"Real_transformer_equivalent_circuit_figure\\\\\"] = 1,\\\\n    [\\\\\"Tap_(transformer)\\\\\"] = 1,\\\\n    [\\\\\"Transformer_universal_EMF_equation\\\\\"] = 1,\\\\n    [\\\\\"stray\\\\\"] = 1,\\\\n}\\\\ntemplate_list = table#1 {\\\\n    [\\\\\"!\\\\\"] = 3,\\\\n    [\\\\\"About\\\\\"] = 1,\\\\n    [\\\\\"Anchor\\\\\"] = 5,\\\\n    [\\\\\"Authority control\\\\\"] = 1,\\\\n    [\\\\\"Cite book\\\\\"] = 36,\\\\n    [\\\\\"Cite conference\\\\\"] = 8,\\\\n    [\\\\\"Cite encyclopedia\\\\\"] = 1,\\\\n    [\\\\\"Cite journal\\\\\"] = 14,\\\\n    [\\\\\"Cite web\\\\\"] = 23,\\\\n    [\\\\\"Clear\\\\\"] = 6,\\\\n    [\\\\\"Col div\\\\\"] = 1,\\\\n    [\\\\\"Col div end\\\\\"] = 1,\\\\n    [\\\\\"Colbegin\\\\\"] = 1,\\\\n    [\\\\\"Colend\\\\\"] = 1,\\\\n    [\\\\\"Commons category\\\\\"] = 1,\\\\n    [\\\\\"Dead link\\\\\"] = 1,\\\\n    [\\\\\"Efn\\\\\"] = 5,\\\\n    [\\\\\"Electric transformers\\\\\"] = 1,\\\\n    [\\\\\"Electricity delivery\\\\\"] = 1,\\\\n    [\\\\\"Electronic component\\\\\"] = 1,\\\\n    [\\\\\"EquationRef\\\\\"] = 7,\\\\n    [\\\\\"Harvnb\\\\\"] = 28,\\\\n    [\\\\\"ISBN\\\\\"] = 2,\\\\n    [\\\\\"Infobox electronic component\\\\\"] = 1,\\\\n    [\\\\\"Main\\\\\"] = 3,\\\\n    [\\\\\"Notelist\\\\\"] = 1,\\\\n    [\\\\\"Nowrap\\\\\"] = 1,\\\\n    [\\\\\"NumBlk\\\\\"] = 7,\\\\n    [\\\\\"Reflist\\\\\"] = 1,\\\\n    [\\\\\"Rp\\\\\"] = 4,\\\\n    [\\\\\"See also\\\\\"] = 1,\\\\n    [\\\\\"Short description\\\\\"] = 1,\\\\n    [\\\\\"Side box\\\\\"] = 1,\\\\n    [\\\\\"TOC limit\\\\\"] = 1,\\\\n    [\\\\\"Ubl\\\\\"] = 1,\\\\n    [\\\\\"Wikibooks\\\\\"] = 1,\\\\n    [\\\\\"\\\\\\\\\\\\\\\\mathrm{d\\\\\"] = 1,\\\\n}\\\\narticle_whitelist = table#1 {\\\\n}\\\\nciteref_patterns = table#1 {\\\\n}\\\\n\"},\"cachereport\":{\"origin\":\"mw-web.codfw.main-58d4fd4d5d-pls2x\",\"timestamp\":\"20250820024216\",\"ttl\":2592000,\"transientcontent\":false}}});});  \\n{\"@context\":\"https:\\\\/\\\\/schema.org\",\"@type\":\"Article\",\"name\":\"Transformer\",\"url\":\"https:\\\\/\\\\/en.wikipedia.org\\\\/wiki\\\\/Transformer\",\"sameAs\":\"http:\\\\/\\\\/www.wikidata.org\\\\/entity\\\\/Q11658\",\"mainEntity\":\"http:\\\\/\\\\/www.wikidata.org\\\\/entity\\\\/Q11658\",\"author\":{\"@type\":\"Organization\",\"name\":\"Contributors to Wikimedia projects\"},\"publisher\":{\"@type\":\"Organization\",\"name\":\"Wikimedia Foundation, Inc.\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\\\/\\\\/www.wikimedia.org\\\\/static\\\\/images\\\\/wmf-hor-googpub.png\"}},\"datePublished\":\"2001-10-12T17:25:38Z\",\"dateModified\":\"2025-08-06T22:17:03Z\",\"image\":\"https:\\\\/\\\\/upload.wikimedia.org\\\\/wikipedia\\\\/commons\\\\/f\\\\/f4\\\\/Philips_N4422_-_power_supply_transformer-2098.jpg\",\"headline\":\"electrical device that transfers energy through electromagnetic induction from one circuit to another circuit\"}')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Transformer\"\n",
    "\n",
    "header_to_split_on = [\n",
    "    (\"h1\", \"Header1\"),\n",
    "    (\"h2\", \"Header2\"),\n",
    "    (\"h3\", \"Header3\"),\n",
    "    (\"h4\", \"Header4\")\n",
    "]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(header_to_split_on)\n",
    "html_header_splits = html_splitter.split_text_from_url(url)\n",
    "html_header_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6689d6f4",
   "metadata": {},
   "source": [
    "4. Recursive JSON Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96f741dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'openapi': '3.1.0', 'info': {'title': 'LangSmith', 'version': '0.1.0'}, 'paths': {'/api/v1/sessions/{session_id}/dashboard': {'post': {'tags': ['tracer-sessions'], 'summary': 'Get Tracing Project Prebuilt Dashboard', 'description': 'Get a prebuilt dashboard for a tracing project.'}}}}\n",
      "{'paths': {'/api/v1/sessions/{session_id}/dashboard': {'post': {'operationId': 'get_tracing_project_prebuilt_dashboard_api_v1_sessions__session_id__dashboard_post', 'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}}}}\n",
      "{'paths': {'/api/v1/sessions/{session_id}/dashboard': {'post': {'parameters': [{'name': 'session_id', 'in': 'path', 'required': True, 'schema': {'type': 'string', 'format': 'uuid', 'title': 'Session Id'}}, {'name': 'accept', 'in': 'header', 'required': False, 'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Accept'}}]}}}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "json_data = requests.get(\"https://api.smith.langchain.com/openapi.json\").json()\n",
    "\n",
    "\n",
    "from langchain_text_splitters import RecursiveJsonSplitter \n",
    "json_splitter = RecursiveJsonSplitter(max_chunk_size=300)\n",
    "json_chunks = json_splitter.split_json(json_data) \n",
    "\n",
    "for chunks in json_chunks[:3]:\n",
    "    print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be38e7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='{\"openapi\": \"3.1.0\", \"info\": {\"title\": \"LangSmith\", \"version\": \"0.1.0\"}, \"paths\": {\"/api/v1/sessions/{session_id}/dashboard\": {\"post\": {\"tags\": [\"tracer-sessions\"], \"summary\": \"Get Tracing Project Prebuilt Dashboard\", \"description\": \"Get a prebuilt dashboard for a tracing project.\"}}}}'\n",
      "page_content='{\"paths\": {\"/api/v1/sessions/{session_id}/dashboard\": {\"post\": {\"operationId\": \"get_tracing_project_prebuilt_dashboard_api_v1_sessions__session_id__dashboard_post\", \"security\": [{\"API Key\": []}, {\"Tenant ID\": []}, {\"Bearer Auth\": []}]}}}}'\n",
      "page_content='{\"paths\": {\"/api/v1/sessions/{session_id}/dashboard\": {\"post\": {\"parameters\": [{\"name\": \"session_id\", \"in\": \"path\", \"required\": true, \"schema\": {\"type\": \"string\", \"format\": \"uuid\", \"title\": \"Session Id\"}}, {\"name\": \"accept\", \"in\": \"header\", \"required\": false, \"schema\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"null\"}], \"title\": \"Accept\"}}]}}}}'\n"
     ]
    }
   ],
   "source": [
    "docs = json_splitter.create_documents([json_data])\n",
    "for doc in docs[:3]:\n",
    "    print(doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
